{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbMYFP9SLI88",
        "outputId": "c2a701ac-3c41-49bb-d262-3fb363b780d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8yIrcNBLgBx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import exp\n",
        "from math import log\n",
        "numpy_rng = np.random.RandomState(1234)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aG-WhEfLrhM"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/Fashion_MNIST/fashion-mnist_train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Fashion_MNIST/fashion-mnist_test.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55vwJnmnL4kE"
      },
      "source": [
        "train_data_df = train_df.drop('label', axis=1)\n",
        "train_data_np = train_data_df.to_numpy()\n",
        "train_bin = np.where(train_data_np > 127, 1, 0)\n",
        "train_data = train_bin.reshape(train_bin.shape[0], 784)\n",
        "train_labels = train_df['label']\n",
        "train_data,val_data,train_labels,val_labels = train_test_split(train_data, train_labels, test_size=0.1, random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWv07zMswgqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eeba1b7-35ca-4434-9f02-d988538bad45"
      },
      "source": [
        "val_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMg_hs2_RV8j"
      },
      "source": [
        "test_data_df = test_df.drop('label', axis=1)\n",
        "test_data_np = test_data_df.to_numpy()\n",
        "test_bin = np.where(test_data_np > 127, 1, 0)\n",
        "test_data = test_bin.reshape(test_bin.shape[0], 784)\n",
        "test_labels = test_df['label']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpwMWgzK2aBr"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1. / (1 + np.exp(-x))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixar8AQm8ISS"
      },
      "source": [
        "def eval(data,W,b):\n",
        "  return softmax(np.reshape(np.matmul(W.T,data),(10,1)) + b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RaR0dzi62Cx"
      },
      "source": [
        "def one_hot_encoding(l, L):\n",
        "  e = []\n",
        "  for i in range(L):\n",
        "    if i == l:\n",
        "      e.append(1)\n",
        "    else:\n",
        "      e.append(0)\n",
        "  e_y = np.asarray(e)\n",
        "  return np.reshape(e_y,(10,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R34B08Li8d9v"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ1rOnOqO7YD"
      },
      "source": [
        "def train(data,labels,W,b,lr = 0.1):\n",
        "\n",
        "  t = []\n",
        "  \n",
        "  for j in range(data.shape[0]):\n",
        "    t.append(one_hot_encoding(labels[j],10))\n",
        "    #print(o[j])\n",
        "    #print(o)\n",
        "  for i in range(50):\n",
        "    grad_w = np.zeros([W.shape[0],W.shape[1]])\n",
        "    grad_b = np.zeros([b.shape[0],1])\n",
        "    o = []\n",
        "    for j in range(data.shape[0]):\n",
        "      o.append(eval(data[j],W,b))\n",
        "      mul2 = np.reshape((o[j]-t[j]),[10,1])\n",
        "      mul1 = np.reshape(data[j],[data[j].shape[0],1])\n",
        "      grad_w += np.matmul(mul1,mul2.T)\n",
        "      grad_b += mul2\n",
        "      W -= lr*grad_w\n",
        "      b -= lr*grad_b\n",
        "      grad_w = np.zeros([W.shape[0],W.shape[1]])\n",
        "      grad_b = np.zeros([b.shape[0],1])\n",
        "  return W,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQi36iP0Nt1L"
      },
      "source": [
        "def test(data,labels,W,b):\n",
        "  cross_entropy = 0;\n",
        "  count = 0;\n",
        "  t = []\n",
        "  o = []\n",
        "  for j in range(data.shape[0]):\n",
        "    o.append(eval(data[j],W,b))\n",
        "    t.append(one_hot_encoding(labels[j],10))\n",
        "    \n",
        "    index_t = np.argmax(t[j],axis = 0)\n",
        "    index_o = np.argmax(o[j],axis = 0)\n",
        "    #print(index,o[j],t[j])\n",
        "   # print(index, o[j])\n",
        "    if index_t == index_o:\n",
        "      count += 1\n",
        "  #  print(t[j][index],o[j][index])\n",
        "   # print(o[j][index_t])\n",
        "    cross_entropy -= log(o[j][index_t]+1e-300)\n",
        "  accuracy = count\n",
        "  cross_entropy = cross_entropy/data.shape[0]\n",
        "\n",
        "  return cross_entropy, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yS_uRSu09A7"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,log_loss\n",
        "def train_test(x_train, y_train, x_test,y_test):\n",
        "  logisticRegr = LogisticRegression(max_iter = 1000,multi_class='multinomial')\n",
        "  logisticRegr.fit(x_train, y_train)\n",
        "  #print(logisticRegr.classes_)\n",
        "  y_pred = logisticRegr.predict(x_test)\n",
        "  pred = logisticRegr.predict_proba(x_test)\n",
        "  #print(y_pred)\n",
        "  #print(y_test)\n",
        "  loss = log_loss(y_test,pred)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  return loss, accuracy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi50r4QphNxf"
      },
      "source": [
        "class RBM(object):\n",
        "    def __init__(self, input=None, n_visible=2, n_hidden=3, W=None, hbias=None, vbias=None):\n",
        "        \n",
        "        if W is None:\n",
        "            initial_W = np.random.randn(n_visible, n_hidden) * 0.01\n",
        "            W = initial_W\n",
        "\n",
        "        if hbias is None:\n",
        "            hbias = np.random.randn(n_hidden) * 0.01  # initialize h bias 0\n",
        "\n",
        "        if vbias is None:\n",
        "            vbias = np.random.randn(n_visible) * 0.01 # initialize v bias 0\n",
        "\n",
        "        self.n_visible = n_visible \n",
        "        self.n_hidden = n_hidden   \n",
        "        self.input = input\n",
        "        self.W = W\n",
        "        self.hbias = hbias\n",
        "        self.vbias = vbias\n",
        "\n",
        "def contrastive_divergence(rbm, lr=0.1, k=1, input=None):\n",
        "    if input is not None:\n",
        "        rbm.input = input\n",
        "    \n",
        "\n",
        "    ph_mean, ph_sample = sample_h_given_v(rbm,rbm.input)\n",
        "\n",
        "    chain_start = ph_sample\n",
        "\n",
        "    for step in range(k):\n",
        "        if step == 0:\n",
        "            nv_means, nv_samples,\\\n",
        "            nh_means, nh_samples = gibbs_hvh(rbm,chain_start)\n",
        "        else:\n",
        "            nv_means, nv_samples,\\\n",
        "            nh_means, nh_samples = gibbs_hvh(rbm,nh_samples)\n",
        "\n",
        "    rbm.W += lr * (np.matmul(rbm.input.T, ph_mean) - np.matmul(nv_samples.T,nh_means))/rbm.input.shape[0]\n",
        "    rbm.vbias += lr * np.mean(rbm.input - nv_samples, axis=0)\n",
        "    rbm.hbias += lr * np.mean(ph_mean - nh_means, axis=0)\n",
        "\n",
        "def sample_h_given_v(rbm, v0_sample):\n",
        "    h1_mean = propup(rbm,v0_sample)\n",
        "    h1_sample = numpy_rng.binomial(size=h1_mean.shape, n=1, p=h1_mean)\n",
        "    return [h1_mean, h1_sample]\n",
        "\n",
        "\n",
        "def sample_v_given_h(rbm, h0_sample):\n",
        "    v1_mean = propdown(rbm,h0_sample)\n",
        "    v1_sample = numpy_rng.binomial(size=v1_mean.shape, n=1, p=v1_mean) \n",
        "    return [v1_mean, v1_sample]\n",
        "\n",
        "def propup(rbm, v):\n",
        "    pre_sigmoid_activation = np.dot(v, rbm.W) + rbm.hbias\n",
        "    return sigmoid(pre_sigmoid_activation)\n",
        "\n",
        "def propdown(rbm, h):\n",
        "    pre_sigmoid_activation = np.dot(h, rbm.W.T) + rbm.vbias\n",
        "    return sigmoid(pre_sigmoid_activation)\n",
        "\n",
        "\n",
        "def gibbs_hvh(rbm, h0_sample):\n",
        "\n",
        "    v1_mean, v1_sample = sample_v_given_h(rbm,h0_sample)\n",
        "    h1_mean, h1_sample = sample_h_given_v(rbm,v1_sample)\n",
        "\n",
        "    return [v1_mean, v1_sample,\n",
        "            h1_mean, h1_sample]\n",
        "\n",
        "def get_hidden_reps(rbm, v,t):\n",
        "    h_v = sigmoid(np.dot(v, rbm.W) + rbm.hbias)\n",
        "    h_t = sigmoid(np.dot(t, rbm.W) + rbm.hbias)\n",
        "    #reconstructed_v = sigmoid(np.dot(h, rbm.W.T) + rbm.vbias)\n",
        "    return h_v,h_t\n",
        "        \n",
        "\n",
        "def train_rbm(rbm, v,t,learning_rate=0.05, k = 1):\n",
        "    contrastive_divergence(rbm,lr=learning_rate,k=k)\n",
        "    return get_hidden_reps(rbm,v,t)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTQ9Oyn84DpU"
      },
      "source": [
        "def swp():\n",
        "    hyperparameter_defaults = dict(\n",
        "      n_hidden=64,\n",
        "      k = 5\n",
        "    )\n",
        "\n",
        "    wandb.init(project=\"Assignment - 04\", config=hyperparameter_defaults)\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"{}_h_layers_{}_steps\".format(config.n_hidden, config.k)\n",
        "    n_visible=784\n",
        "    rbm = RBM(input=train_data, n_visible=n_visible, n_hidden=config.n_hidden)\n",
        "\n",
        "    for epoch in range(25):\n",
        "\n",
        "      hidden_rep,hidden_rep_test= train_rbm(rbm, val_data,test_data,k=config.k)\n",
        "      print('Training epoch: %d' %epoch)\n",
        "      \n",
        "      #W_l, b_l = train(hidden_rep,val_labels,W_l,b_l,lr=0.1)\n",
        "      loss, accuracy = train_test(hidden_rep, val_labels, hidden_rep_test,test_labels)\n",
        "      print('Loss:{} Accuracy:{}'.format(loss,accuracy))\n",
        "      metrics = {'epoch':epoch, 'accuracy': val_accuracy, 'loss': loss}\n",
        "      wandb.log(metrics)\n",
        "    wandb.run.finish()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhXh0oIRuYlE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "67e0af6b-32f3-4b5b-9574-911021fd429f"
      },
      "source": [
        "swp()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8f7ff6974b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mswp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-c593289ce2fa>\u001b[0m in \u001b[0;36mswp\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Assignment - 04\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}_h_layers_{}_steps\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    }
  ]
}