{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6910_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodikarthik21/CS6910---Fundamentals-of-Deep-Learning/blob/main/Assignment01/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRwBqsxQoczw",
        "outputId": "501c0d7d-dad4-4188-b4fd-1a94e18ae4b1"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.9MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=8c3d9dc2e4d9fe7b8e45964e1602f7b9801d6b5c8a73a03901de287d4c25b0a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=6900fbcb5c4c5607dd32a21e62b7132585249c3b3dc3f9ceb694ede144ef28bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: sentry-sdk, docker-pycreds, smmap, gitdb, GitPython, pathtools, subprocess32, shortuuid, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ods22NlvpAfG",
        "outputId": "2d269071-ea49-4705-cad4-ceebb6ecbc37"
      },
      "source": [
        "import wandb\r\n",
        "wandb.login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edFb4Fygf0hv"
      },
      "source": [
        "#Question 1 (2 Marks)\r\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use \"from keras.datasets import fashion_mnist\" for getting the fashion mnist dataset.\r\n",
        "\r\n",
        "﻿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9ygxnE_ctp"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot\r\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "label = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "for i in range(10):\r\n",
        "   wandb.init(project=\"cs6910-assignment01\")\r\n",
        "   wandb.run.name = \"Q1_run_{}\".format(i+1)\r\n",
        "   for j in range(30):\r\n",
        "     if y_train[j] == i:\r\n",
        "       wandb.log({\"examples\": [wandb.Image(X_train[j], caption=label[i])]})\r\n",
        "       break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DMgxgi4PRGt"
      },
      "source": [
        "# Question 2 (10 Marks)\r\n",
        "\r\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\r\n",
        "\r\n",
        "Your code should be flexible so that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3thNIX-cOIA3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(z):\r\n",
        "  g = 1/(1+np.exp(-z))\r\n",
        "  return g\r\n",
        "\r\n",
        "def tanh(z):\r\n",
        "  g = np.tanh(z)\r\n",
        "  return g\r\n",
        "\r\n",
        "def relu(z):\r\n",
        "  return np.maximum(0,z)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_qU08dQy3f5"
      },
      "source": [
        "def softmax(x):\r\n",
        "    e_x = np.exp(x - np.max(x))\r\n",
        "    return e_x / e_x.sum()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXeouW-aXGsb"
      },
      "source": [
        "def initialize(initializer, size1, size2):\r\n",
        "  if(initializer == \"random\"):\r\n",
        "     W = np.random.randn(size1, size2) * 0.01\r\n",
        "     return W\r\n",
        "\r\n",
        "  if(initializer == \"Xavier\"):\r\n",
        "     W = np.random.randn(size1, size2) * np.sqrt(1/size2)\r\n",
        "     return W\r\n",
        "\r\n",
        "  print(\"Enter the name of initializer correctly\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6P2Su_1dv30"
      },
      "source": [
        "def linear_forward(H, W, b):\r\n",
        "  W = np.asarray(W)\r\n",
        "  H = np.reshape(H,(H.shape[0],-1))\r\n",
        "  A = np.dot(W,H) + b \r\n",
        "  cache = (H, W, b)\r\n",
        "  \r\n",
        "  return A, cache"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zXc1-js-dgL"
      },
      "source": [
        "def initialize_Wb_matrix(X, num_hidden, size_hidden, initializer):\r\n",
        "  layer_dims = [X.shape[0]]\r\n",
        "  for l in range(0, num_hidden):\r\n",
        "    layer_dims.append(size_hidden)\r\n",
        "  layer_dims.append(10)   \r\n",
        "  np.random.seed(3)\r\n",
        "  Wb_matrix = {}\r\n",
        "  update = {}\r\n",
        "  grads = {}\r\n",
        "  L = len(layer_dims)            # number of layers in the network\r\n",
        "\r\n",
        "  for l in range(1,L):\r\n",
        "    Wb_matrix['W' + str(l)] = initialize(initializer, layer_dims[l], layer_dims[l-1])\r\n",
        "    update['W' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\r\n",
        "    Wb_matrix['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "    update['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "    grads['dW' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\r\n",
        "    grads['db' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "  return Wb_matrix, update, grads"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7zsskSyU1ZO"
      },
      "source": [
        "def activation_forward(H_prev, W, b, activation):\r\n",
        "  A, linear_cache = linear_forward(H_prev, W, b)\r\n",
        "  activation_cache = A\r\n",
        "  if activation == 'relu':\r\n",
        "    H = relu(A)\r\n",
        "  elif activation == 'sigmoid':\r\n",
        "    H = sigmoid(A)\r\n",
        "  elif activation =='tanh':\r\n",
        "    H = tanh(A)\r\n",
        "  elif activation == 'softmax':\r\n",
        "    H = softmax(A)\r\n",
        "  \r\n",
        "  return H, activation_cache, linear_cache"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DITldZOSHNt2"
      },
      "source": [
        "def forward_propagation(X, Wb_matrix, activation):\r\n",
        "    H = X\r\n",
        "    L = int((len(Wb_matrix)/2))\r\n",
        "    A_caches = []\r\n",
        "    H_caches = [H]\r\n",
        "    for l in range(1, L):\r\n",
        "        H_prev = H \r\n",
        "        H, A_cache, linear_cache = activation_forward(H_prev, Wb_matrix['W{:d}'.format(l)], Wb_matrix['b{:d}'.format(l)], activation)\r\n",
        "        A_caches.append(A_cache)\r\n",
        "        H_caches.append(H)\r\n",
        "    HL, AL, linear_cache = activation_forward(H, Wb_matrix['W%d' % L], Wb_matrix['b%d' % L], activation='softmax')\r\n",
        "    A_caches.append(AL)\r\n",
        "    H_caches.append(HL)\r\n",
        "    return HL, H_caches, A_caches"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6hlMq4saR14"
      },
      "source": [
        "# Question 3 (18 Marks)\r\n",
        "\r\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions \r\n",
        "\r\n",
        "- sgd\r\n",
        "- momentum based gradient descent\r\n",
        "- nesterov accelerated gradient descent\r\n",
        "- rmsprop\r\n",
        "- adam\r\n",
        "- nadam\r\n",
        "\r\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\r\n",
        "\r\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3Qc4pcN2iD"
      },
      "source": [
        "def one_hot_encoding(l, L):\r\n",
        "  import numpy\r\n",
        "  e = []\r\n",
        "  for i in range(L):\r\n",
        "    if i == l:\r\n",
        "      e.append(1)\r\n",
        "    else:\r\n",
        "      e.append(0)\r\n",
        "  e_y = np.asarray(e)\r\n",
        "  return e_y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6UpazPvcV6c"
      },
      "source": [
        "def deriv_activ(z, activation):\r\n",
        "  if(activation == \"relu\"):\r\n",
        "    z[z<=0] = 0\r\n",
        "    z[z>0] = 1\r\n",
        "    return z\r\n",
        "\r\n",
        "  elif(activation == \"sigmoid\"):\r\n",
        "    g_deriv = sigmoid(z) * (1 - sigmoid(z))\r\n",
        "    return g_deriv\r\n",
        "  \r\n",
        "  elif(activation == \"tanh\"):\r\n",
        "    deriv = 1 - (tanh(z)) ** 2\r\n",
        "    return deriv"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGp9a-HAVJaZ"
      },
      "source": [
        "def back_propagation(H_caches, A_caches, Wb_matrix, y_hat, y, activation, weight_decay):\r\n",
        "  e_y = []\r\n",
        "  L = int((len(Wb_matrix)/2))\r\n",
        "  num_train = y_hat.shape[1]\r\n",
        "  num_classes = y_hat.shape[0]\r\n",
        "  for j in range(len(y)):\r\n",
        "    e_y.append(one_hot_encoding(y[j],num_classes))\r\n",
        "  e_y = np.reshape(e_y,(num_classes,num_train))\r\n",
        "  da = -(e_y - y_hat)\r\n",
        "  grads = {}\r\n",
        "  \r\n",
        "  for k in reversed(range(1,L+1)):\r\n",
        "    dW = np.matmul(da, np.transpose(H_caches[k-1])) + 2*weight_decay*Wb_matrix['W{:d}'.format(k)]\r\n",
        "    db = da\r\n",
        "    if k != 1:\r\n",
        "      dh = np.dot(Wb_matrix['W{:d}'.format(k)].T, da)\r\n",
        "      da = np.multiply(dh, deriv_activ(A_caches[k-2], activation))\r\n",
        "    grads[\"dW\" + str(k)] = dW\r\n",
        "    grads[\"db\" + str(k)] = db\r\n",
        "  \r\n",
        "  return grads"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l16UXHD_YsCH"
      },
      "source": [
        "def sgd(Wb_matrix, grads, eta,L):\r\n",
        "  for l in range(L):\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - eta * grads[\"dW\" + str(l+1)]\r\n",
        "    Wb_matrix['b' + str(l+1)] = Wb_matrix['b' + str(l+1)] - eta * grads[\"db\" + str(l+1)]\r\n",
        "  return Wb_matrix\r\n",
        "\r\n",
        "def momentum(Wb_matrix, grads, eta, gamma, update,L):\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = (gamma * update['W' + str(l+1)]) + eta * grads['dW' + str(l+1)]\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - update['W' + str(l+1)]\r\n",
        "\r\n",
        "    update[\"b\" + str(l+1)] = (gamma * update[\"b\" + str(l+1)]) + eta * grads[\"db\" + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - update[\"b\" + str(l+1)]\r\n",
        "  \r\n",
        "  return Wb_matrix, update\r\n",
        "\r\n",
        "def nag(Wb_matrix,eta,gamma, update, L):\r\n",
        "  Wb_matrix_temp= {}\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = (gamma * update['W' + str(l+1)])\r\n",
        "    Wb_matrix_temp['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - update['W' + str(l+1)]\r\n",
        "\r\n",
        "    update[\"b\" + str(l+1)] = (gamma * update[\"b\" + str(l+1)])\r\n",
        "    Wb_matrix_temp[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - update[\"b\" + str(l+1)]\r\n",
        "\r\n",
        "  return Wb_matrix_temp, update\r\n",
        "\r\n",
        "def rmsprop(Wb_matrix, eta, update ,beta1, eps, grads, L):\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = beta1 * update['W' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]**2\r\n",
        "    update[\"b\" + str(l+1)] = beta1 * update[\"b\" + str(l+1)] + (1-beta1) * grads[\"db\" + str(l+1)]**2\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - (eta/np.sqrt(update['W' + str(l+1)]+eps)) * grads['dW' + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - (eta/np.sqrt(update['b' + str(l+1)]+eps)) * grads['db' + str(l+1)]\r\n",
        "  return Wb_matrix, update\r\n",
        "  \r\n",
        "def adam(Wb_matrix, eta, beta1, beta2, update, v_n, eps, grads, L,i):\r\n",
        "  \r\n",
        "  m_hat = update.copy()\r\n",
        "  v_hat = update.copy()\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = beta1 * update['W' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]\r\n",
        "    update['b' + str(l+1)] = beta1 * update['b' + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\r\n",
        "    \r\n",
        "    v_n['W' + str(l+1)] = beta2 * v_n['W' + str(l+1)] + (1-beta2) * grads['dW' + str(l+1)]**2\r\n",
        "    v_n[\"b\" + str(l+1)] = beta2 * v_n[\"b\" + str(l+1)] + (1-beta2) * grads['db' + str(l+1)]**2\r\n",
        "    \r\n",
        "    m_hat['W' + str(l+1)] = update['W' + str(l+1)]/(1-math.pow(beta1,i+1))\r\n",
        "    m_hat['b' + str(l+1)] = update['b' + str(l+1)]/(1-math.pow(beta1,i+1))\r\n",
        "\r\n",
        "    v_hat['W' + str(l+1)] = v_n['W' + str(l+1)]/(1-math.pow(beta2,i+1))\r\n",
        "    v_hat['b' + str(l+1)] = v_n['b' + str(l+1)]/(1-math.pow(beta2,i+1))\r\n",
        "\r\n",
        "\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - (eta/(np.sqrt(v_hat['W' + str(l+1)]+eps)))* m_hat['W' + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - (eta/(np.sqrt(v_hat['b' + str(l+1)]+eps)))* m_hat['b' + str(l+1)]\r\n",
        "\r\n",
        " \r\n",
        "  return Wb_matrix, update, v_n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo0kRUHIQMDO"
      },
      "source": [
        "def swp():\r\n",
        "  from keras.datasets import fashion_mnist\r\n",
        "  from matplotlib import pyplot\r\n",
        "  import numpy as np\r\n",
        "  import math\r\n",
        "  import random\r\n",
        "  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "  X_train_flatten = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))/255.0\r\n",
        "  X_train_flatten = X_train_flatten.T\r\n",
        "  num_train = 10000\r\n",
        "  num_val = int(0.1*num_train)\r\n",
        "  \r\n",
        "  val = random.sample(range(int(num_train)), int(num_val))\r\n",
        "\r\n",
        "  X_val = np.zeros([X_train.shape[1]*X_train.shape[2], num_val])\r\n",
        "  y_val = np.zeros(num_val)\r\n",
        "  for a in range(num_val):\r\n",
        "    X_val[:,a] = X_train_flatten[:,int(val[a])]\r\n",
        "    y_val[a] = y_train[int(val[a])]\r\n",
        "\r\n",
        "  hyperparameter_defaults = dict(\r\n",
        "      num_hidden = 4,\r\n",
        "      size_hidden = 128,\r\n",
        "      weight_decay = 0,\r\n",
        "      optimizer = 'adam',\r\n",
        "      initializer = 'Xavier',\r\n",
        "      activation = 'relu',\r\n",
        "      batch_size = 64,\r\n",
        "      learning_rate = 0.001,\r\n",
        "      max_epoch = 5,\r\n",
        "      )\r\n",
        "\r\n",
        "\r\n",
        "  wandb.init(project=\"cs6910-assignment01\", config=hyperparameter_defaults)\r\n",
        "  config = wandb.config\r\n",
        "  wandb.run.name = \"hl{}_bs_{}_ac_{}\".format(config.num_hidden,config.batch_size, config.activation)\r\n",
        "  L = config.num_hidden+1\r\n",
        "  beta1 = 0.9\r\n",
        "  beta2 = 0.999\r\n",
        "  eps = 1e-8\r\n",
        "  gamma = 0.9\r\n",
        "\r\n",
        "\r\n",
        "  Wb_matrix, update, grad_initial = initialize_Wb_matrix(X_train_flatten, config.num_hidden , config.size_hidden ,config.initializer)\r\n",
        "  v = update.copy()\r\n",
        "  m_hat = update.copy()\r\n",
        "  v_hat = update.copy()\r\n",
        "\r\n",
        "  \r\n",
        "  for i in range(config.max_epoch):\r\n",
        "    loss = 0;\r\n",
        "    val_loss = 0;\r\n",
        "    count = 0;\r\n",
        "    grads = grad_initial\r\n",
        "    print(\"Epoch:\", i+1)\r\n",
        "    for j in range(num_train):\r\n",
        "      train_ip= np.reshape(X_train_flatten[:,j], (-1, 1)) \r\n",
        "      y_hat, H_caches, A_caches = forward_propagation(train_ip, Wb_matrix ,config.activation)\r\n",
        "      if(y_train[j]==np.argmax(y_hat)):\r\n",
        "        count = count + 1; \r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0  :\r\n",
        "        if config.optimizer == 'nag':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "\r\n",
        "      grad = back_propagation(H_caches, A_caches, Wb_matrix, y_hat, [y_train[j]], config.activation, config.weight_decay)\r\n",
        "      for l in range(L):\r\n",
        "        grads['dW' + str(l+1)] = grads['dW' + str(l+1)] + grad['dW' + str(l+1)] \r\n",
        "        grads['db' + str(l+1)] = grads['db' + str(l+1)] + grad['db' + str(l+1)] \r\n",
        "\r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0 : \r\n",
        "        for l in range(L):\r\n",
        "          grads['dW' + str(l+1)] = grads['dW' + str(l+1)]/config.batch_size\r\n",
        "          grads['db' + str(l+1)] = grads['db' + str(l+1)]/config.batch_size\r\n",
        "        if config.optimizer == 'sgd':\r\n",
        "          Wb_matrix = sgd(Wb_matrix, grads, config.learning_rate,L)\r\n",
        "        elif config.optimizer == 'momentum':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'nag':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'rmsprop':\r\n",
        "          Wb_matrix, update = rmsprop(Wb_matrix, config.learning_rate, update,  beta1, eps, grads, L)\r\n",
        "        elif config.optimizer == 'adam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)    \r\n",
        "      \r\n",
        "      loss = loss - (1/num_train)*math.log(y_hat[y_train[j]])\r\n",
        "    \r\n",
        "    y_hat_val, H_caches_val, A_caches_val = forward_propagation(X_val, Wb_matrix ,config.activation)\r\n",
        "    count_val = np.sum(np.argmax(y_hat_val, axis = 0)== y_val)\r\n",
        "    \r\n",
        "    for j in range(num_val):\r\n",
        "      val_ip= np.reshape(X_val[:,j], (-1, 1)) \r\n",
        "      y_hat_val, H_caches_val, A_caches_val = forward_propagation(val_ip, Wb_matrix ,config.activation)\r\n",
        "      val_loss = val_loss - (1/num_val)*math.log(y_hat_val[int(y_val[j])])\r\n",
        "\r\n",
        "    accuracy = 100*count/num_train\r\n",
        "    val_accuracy = 100*count_val/num_val\r\n",
        "\r\n",
        "    print(\"     Loss\", loss)\r\n",
        "    print(\"     Accuracy\",accuracy)\r\n",
        "    print(\"     Validation Loss\", val_loss)\r\n",
        "    print(\"     Validation Accuracy\", val_accuracy)\r\n",
        "\r\n",
        "    metrics = {'epoch':i, 'val_accuracy': val_accuracy, 'val_loss': val_loss, 'accuracy': accuracy, 'loss': loss}\r\n",
        "    wandb.log(metrics)\r\n",
        "  wandb.run.finish()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXXTVz-EB8iT",
        "outputId": "a2180fb7-70df-403b-9993-98ec3ff5924d"
      },
      "source": [
        "import wandb\r\n",
        "\r\n",
        "sweep_config = {\r\n",
        "  \"name\": \"My Sweep\",\r\n",
        "  \"method\": \"random\",\r\n",
        "  \"project\": \"cs6910-assignment01\",\r\n",
        "  \"metric\":{\r\n",
        "      \"name\":\"val_accuracy\",\r\n",
        "      \"goal\":\"maximize\"\r\n",
        "  },\r\n",
        "  \"parameters\": {\r\n",
        "        \"max_epoch\": {\r\n",
        "            \"values\": [5,10]\r\n",
        "        },\r\n",
        "        \"num_hidden\": {\r\n",
        "            \"values\":[3,4,5]\r\n",
        "        }, \r\n",
        "        \"size_hidden\": {\r\n",
        "            \"values\":[32,64,128]\r\n",
        "        },\r\n",
        "        \"weight_decay\":{\r\n",
        "            \"values\":[0,0.0005,0.5]\r\n",
        "        },\r\n",
        "        \"learning_rate\":{\r\n",
        "            \"values\":[0.001,0.0001]\r\n",
        "        },\r\n",
        "        \"batch_size\": {\r\n",
        "            \"values\":[16,32,64]\r\n",
        "        },  \r\n",
        "        \"optimizer\": {\r\n",
        "            \"values\":['momentum' ,'nag','rmsprop', 'adam','nadam']\r\n",
        "        },\r\n",
        "        \"initializer\": {\r\n",
        "            \"values\":['Xavier']\r\n",
        "        },\r\n",
        "        \"activation\":{\r\n",
        "            \"values\": ['relu']\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 50p0wqep\n",
            "Sweep URL: https://wandb.ai/karthik21/uncategorized/sweeps/50p0wqep\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "id": "ySONBOV_DXnm",
        "outputId": "3516092d-35c3-4993-a1ec-e5f3c54913f1"
      },
      "source": [
        "wandb.agent(sweep_id, swp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cnih3to9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_hidden: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik21\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">proud-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karthik21/uncategorized\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karthik21/uncategorized/sweeps/50p0wqep\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized/sweeps/50p0wqep</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karthik21/uncategorized/runs/cnih3to9\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized/runs/cnih3to9</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210312_094623-cnih3to9</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "     Loss 0.9712336700058178\n",
            "     Accuracy 62.97\n",
            "     Validation Loss 0.6806922353253155\n",
            "     Validation Accuracy 75.6\n",
            "Epoch: 2\n",
            "     Loss 0.62235560748704\n",
            "     Accuracy 76.75\n",
            "     Validation Loss 0.5356946377407659\n",
            "     Validation Accuracy 80.3\n",
            "Epoch: 3\n",
            "     Loss 0.5328880562593504\n",
            "     Accuracy 80.45\n",
            "     Validation Loss 0.48568405351031785\n",
            "     Validation Accuracy 81.7\n",
            "Epoch: 4\n",
            "     Loss 0.4814977027028728\n",
            "     Accuracy 82.52\n",
            "     Validation Loss 0.44654699774836065\n",
            "     Validation Accuracy 83.8\n",
            "Epoch: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXINphelC-9O"
      },
      "source": [
        ""
      ]
    }
  ]
}