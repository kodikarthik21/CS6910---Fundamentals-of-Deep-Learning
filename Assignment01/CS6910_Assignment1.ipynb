{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6910_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodikarthik21/CS6910---Fundamentals-of-Deep-Learning/blob/main/Assignment01/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRwBqsxQoczw"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ods22NlvpAfG"
      },
      "source": [
        "import wandb\r\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edFb4Fygf0hv"
      },
      "source": [
        "#Question 1 (2 Marks)\r\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use \"from keras.datasets import fashion_mnist\" for getting the fashion mnist dataset.\r\n",
        "\r\n",
        "ï»¿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9ygxnE_ctp"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot\r\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "label = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "for i in range(10):\r\n",
        "   wandb.init(project=\"cs6910-assignment01\")\r\n",
        "   wandb.run.name = \"Q1_run_{}\".format(i+1)\r\n",
        "   for j in range(30):\r\n",
        "     if y_train[j] == i:\r\n",
        "       wandb.log({\"examples\": [wandb.Image(X_train[j], caption=label[i])]})\r\n",
        "       break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DMgxgi4PRGt"
      },
      "source": [
        "# Question 2 (10 Marks)\r\n",
        "\r\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\r\n",
        "\r\n",
        "Your code should be flexible so that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3thNIX-cOIA3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def sigmoid(z):\r\n",
        "  g = 1/(1+np.exp(-z))\r\n",
        "  return g\r\n",
        "\r\n",
        "def tanh(z):\r\n",
        "  g = np.tanh(z)\r\n",
        "  return g\r\n",
        "\r\n",
        "def relu(z):\r\n",
        "  return np.maximum(0,z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_qU08dQy3f5"
      },
      "source": [
        "def softmax(x):\r\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXeouW-aXGsb"
      },
      "source": [
        "def initialize(initializer, size1, size2):\r\n",
        "  if(initializer == \"random\"):\r\n",
        "     W = np.random.randn(size1, size2)\r\n",
        "     return W\r\n",
        "\r\n",
        "  if(initializer == \"Xavier\"):\r\n",
        "     W = np.random.randn(size1, size2) * np.sqrt(1/size2)\r\n",
        "     return W\r\n",
        "\r\n",
        "  print(\"Enter the name of initializer correctly\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6P2Su_1dv30"
      },
      "source": [
        "def linear_forward(H, W, b, activation):\r\n",
        "  A = np.dot(W, H) + b \r\n",
        "  cache = (H, W, b)\r\n",
        "\r\n",
        "  return A, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7zsskSyU1ZO"
      },
      "source": [
        "def activation_forward(H_prev, W, b, activation):\r\n",
        "  A, linear_cache = linear_forward(H_prev, W, b)\r\n",
        "  activation_cache = A\r\n",
        "  if activation == 'relu':\r\n",
        "    H = relu(A)\r\n",
        "  elif activation == 'sigmoid':\r\n",
        "    H = sigmoid(A)\r\n",
        "  elif activation =='tanh':\r\n",
        "    H = tanh(A)\r\n",
        "  elif activation == 'softmax':\r\n",
        "    H = softmax(A)\r\n",
        "\r\n",
        "  return H, activation_cache, linear_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zXc1-js-dgL"
      },
      "source": [
        "def initialize_Wb_matrix(layer_dims, initializer):\r\n",
        "        \r\n",
        "    np.random.seed(3)\r\n",
        "    Wb_matrix = {}\r\n",
        "    L = len(layer_dims)            # number of layers in the network\r\n",
        "\r\n",
        "    for l in range(1, L):\r\n",
        "        Wb_matrix['W' + str(l)] = initialize(initializer, layer_dims[l], layer_dims[l-1])\r\n",
        "        Wb_matrix['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "    \r\n",
        "    return Wb_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DITldZOSHNt2"
      },
      "source": [
        "def L_layer_model(X, Wb_matrix, activation):\r\n",
        "    H = X\r\n",
        "    L = len(Wb_matrix) // 2\r\n",
        "    A_caches = []\r\n",
        "    H_caches = [H]\r\n",
        "    for l in range(1, L):\r\n",
        "        H_prev = H \r\n",
        "        H, A_cache, linear_cache = activation_forward(H_prev, Wb_matrix['W{:d}'.format(l)], Wb_matrix['b{:d}'.format(l)], activation)\r\n",
        "        A_caches.append(A_cache)\r\n",
        "        H_caches.append(H)\r\n",
        "    HL, A_cache = activation_forward(A, Wb_matrix['W%d' % L], Wb_matrix['b%d' % L], activation='softmax')\r\n",
        "    A_caches.append(A_cache)\r\n",
        "    return HL, H_caches, A_caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83_rbLFKN4et"
      },
      "source": [
        "def forward_propagation(X, num_hidden, size_hidden, initializer, activation):\r\n",
        "\r\n",
        "  layer_dims = [X.shape(0)]\r\n",
        "  for l in range(1, num_hidden):\r\n",
        "    layer_dims.append(size_hidden)\r\n",
        "  layer_dims.append(10)\r\n",
        "  \r\n",
        "  Wb_matrix = initialize_Wb_matrix(layer_dims, initializer)\r\n",
        "  y_hat, H_caches A_caches = L_layer_model(X, Wb_matrix, activation)\r\n",
        "\r\n",
        "  return y_hat, H_caches, A_caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6hlMq4saR14"
      },
      "source": [
        "# Question 3 (18 Marks)\r\n",
        "\r\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions \r\n",
        "\r\n",
        "- sgd\r\n",
        "- momentum based gradient descent\r\n",
        "- nesterov accelerated gradient descent\r\n",
        "- rmsprop\r\n",
        "- adam\r\n",
        "- nadam\r\n",
        "\r\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\r\n",
        "\r\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3Qc4pcN2iD"
      },
      "source": [
        "def one_hot_encoding(l, L):\r\n",
        "  e_y = []\r\n",
        "\r\n",
        "  for i in range(L):\r\n",
        "    if i == l:\r\n",
        "      e_y.append(1)\r\n",
        "    else:\r\n",
        "      e_y.append(0)\r\n",
        "  \r\n",
        "  return np.array(e_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6UpazPvcV6c"
      },
      "source": [
        "def deriv_activ(z, activation):\r\n",
        "  if(activation == \"relu\"):\r\n",
        "    x[x<=0] = 0\r\n",
        "    x[x>0] = 1\r\n",
        "    return x\r\n",
        "\r\n",
        "  elif(activation == \"sigmoid\"):\r\n",
        "    g_deriv = sigmoid(z) * (1 - sigmoid(z))\r\n",
        "    return g_deriv\r\n",
        "  \r\n",
        "  elif(activation == \"tanh\"):\r\n",
        "    deriv = 1 - (tanh(z)) ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGp9a-HAVJaZ"
      },
      "source": [
        "def back_propagation(H_caches, A_caches, Wb_matrix, y_hat, y, activation):\r\n",
        "  e_y = one_hot_encoding(y, L)\r\n",
        "  da = -(e_y - y_hat)\r\n",
        "  grads = {}\r\n",
        "\r\n",
        "  for k in reversed(range(1, L+1)):\r\n",
        "    dW = np.dot(da, H_caches[k-1].T)\r\n",
        "    db = da\r\n",
        "    dh = np.dot(Wb_matrix['W{:d}'.format(k)].T, da)\r\n",
        "    da = np.multiply(dh, deriv_activ(A_caches[k-1], activation))\r\n",
        "    grads[\"dW\" + str(k)] = dW\r\n",
        "    grads[\"db\" + str(k)] = db\r\n",
        "  \r\n",
        "  return dW_caches, db_caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l16UXHD_YsCH"
      },
      "source": [
        "def sgd(Wb_matrix, grads, eta):\r\n",
        "  for l in range(L):\r\n",
        "    Wb_matrix[\"W\" + str(l+1)] = Wb_matrix[\"W\" + str(l+1)] - eta * grads[\"dW\" + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - eta * grads[\"db\" + str(l+1)]\r\n",
        "  return Wb_matrix\r\n",
        "\r\n",
        "def momentum(Wb_matrix, grads, eta, gamma, update):\r\n",
        "  update_new = {}\r\n",
        "  for l in range(L):\r\n",
        "    update_new[\"W\" + str(l+1)] = (gamma * update[\"W\" + str(l+1)]) + eta * grads[\"dW\" + str(l+1)]\r\n",
        "    Wb_matrix[\"W\" + str(l+1)] = Wb_matrix[\"W\" + str(l+1)] - update_new[\"W\" + str(l+1)]\r\n",
        "\r\n",
        "    update_new[\"b\" + str(l+1)] = (gamma * update[\"b\" + str(l+1)]) + eta * grads[\"db\" + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - update_new[\"b\" + str(l+1)]\r\n",
        "  \r\n",
        "  return Wb_matrix, update_new"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}