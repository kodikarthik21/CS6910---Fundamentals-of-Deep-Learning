{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6910_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodikarthik21/CS6910---Fundamentals-of-Deep-Learning/blob/main/Assignment01/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRwBqsxQoczw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e300f7-2dbd-413a-d6ea-7f60186b270b"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.22)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ods22NlvpAfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3faf4ef-fd41-4f71-b976-eb705b634328"
      },
      "source": [
        "import wandb\r\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik21\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edFb4Fygf0hv"
      },
      "source": [
        "#Question 1 (2 Marks)\r\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use \"from keras.datasets import fashion_mnist\" for getting the fashion mnist dataset.\r\n",
        "\r\n",
        "ï»¿"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9ygxnE_ctp"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot\r\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "label = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "for i in range(10):\r\n",
        "   wandb.init(project=\"cs6910-assignment01\")\r\n",
        "   wandb.run.name = \"Q1_run_{}\".format(i+1)\r\n",
        "   for j in range(30):\r\n",
        "     if y_train[j] == i:\r\n",
        "       wandb.log({\"examples\": [wandb.Image(X_train[j], caption=label[i])]})\r\n",
        "       break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DMgxgi4PRGt"
      },
      "source": [
        "# Question 2 (10 Marks)\r\n",
        "\r\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\r\n",
        "\r\n",
        "Your code should be flexible so that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3thNIX-cOIA3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(z):\r\n",
        "  g = 1/(1+np.exp(-z))\r\n",
        "  return g\r\n",
        "\r\n",
        "def tanh(z):\r\n",
        "  g = np.tanh(z)\r\n",
        "  return g\r\n",
        "\r\n",
        "def relu(z):\r\n",
        "  return np.maximum(0,z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_qU08dQy3f5"
      },
      "source": [
        "def softmax(x):\r\n",
        "    e_x = np.exp(x - np.max(x))\r\n",
        "    return e_x / e_x.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXeouW-aXGsb"
      },
      "source": [
        "def initialize(initializer, size1, size2):\r\n",
        "  if(initializer == \"random\"):\r\n",
        "     W = np.random.randn(size1, size2) * 0.01\r\n",
        "     return W\r\n",
        "\r\n",
        "  if(initializer == \"Xavier\"):\r\n",
        "     W = np.random.randn(size1, size2) * np.sqrt(1/size2)\r\n",
        "     return W\r\n",
        "\r\n",
        "  print(\"Enter the name of initializer correctly\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6P2Su_1dv30"
      },
      "source": [
        "def linear_forward(H, W, b):\r\n",
        "  W = np.asarray(W)\r\n",
        "  H = np.reshape(H,(H.shape[0],-1))\r\n",
        "  A = np.dot(W,H) + b \r\n",
        "  cache = (H, W, b)\r\n",
        "  \r\n",
        "  return A, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zXc1-js-dgL"
      },
      "source": [
        "def initialize_Wb_matrix(X, num_hidden, size_hidden, initializer):\r\n",
        "  layer_dims = [X.shape[0]]\r\n",
        "  for l in range(0, num_hidden):\r\n",
        "    layer_dims.append(size_hidden)\r\n",
        "  layer_dims.append(10)   \r\n",
        "  np.random.seed(3)\r\n",
        "  Wb_matrix = {}\r\n",
        "  update = {}\r\n",
        "  grads = {}\r\n",
        "  L = len(layer_dims)            # number of layers in the network\r\n",
        "\r\n",
        "  for l in range(1,L):\r\n",
        "    Wb_matrix['W' + str(l)] = initialize(initializer, layer_dims[l], layer_dims[l-1])\r\n",
        "    update['W' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\r\n",
        "    Wb_matrix['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "    update['b' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "    grads['dW' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\r\n",
        "    grads['db' + str(l)] = np.zeros((layer_dims[l], 1))\r\n",
        "  return Wb_matrix, update, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7zsskSyU1ZO"
      },
      "source": [
        "def activation_forward(H_prev, W, b, activation):\r\n",
        "  A, linear_cache = linear_forward(H_prev, W, b)\r\n",
        "  activation_cache = A\r\n",
        "  if activation == 'relu':\r\n",
        "    H = relu(A)\r\n",
        "  elif activation == 'sigmoid':\r\n",
        "    H = sigmoid(A)\r\n",
        "  elif activation =='tanh':\r\n",
        "    H = tanh(A)\r\n",
        "  elif activation == 'softmax':\r\n",
        "    H = softmax(A)\r\n",
        "  \r\n",
        "  return H, activation_cache, linear_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DITldZOSHNt2"
      },
      "source": [
        "def forward_propagation(X, Wb_matrix, activation):\r\n",
        "    H = X\r\n",
        "    L = int((len(Wb_matrix)/2))\r\n",
        "    A_caches = []\r\n",
        "    H_caches = [H]\r\n",
        "    for l in range(1, L):\r\n",
        "        H_prev = H \r\n",
        "        H, A_cache, linear_cache = activation_forward(H_prev, Wb_matrix['W{:d}'.format(l)], Wb_matrix['b{:d}'.format(l)], activation)\r\n",
        "        A_caches.append(A_cache)\r\n",
        "        H_caches.append(H)\r\n",
        "    HL, AL, linear_cache = activation_forward(H, Wb_matrix['W%d' % L], Wb_matrix['b%d' % L], activation='softmax')\r\n",
        "    A_caches.append(AL)\r\n",
        "    H_caches.append(HL)\r\n",
        "    return HL, H_caches, A_caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6hlMq4saR14"
      },
      "source": [
        "# Question 3 (18 Marks)\r\n",
        "\r\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions \r\n",
        "\r\n",
        "- sgd\r\n",
        "- momentum based gradient descent\r\n",
        "- nesterov accelerated gradient descent\r\n",
        "- rmsprop\r\n",
        "- adam\r\n",
        "- nadam\r\n",
        "\r\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\r\n",
        "\r\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b3Qc4pcN2iD"
      },
      "source": [
        "def one_hot_encoding(l, L):\r\n",
        "  import numpy\r\n",
        "  e = []\r\n",
        "  for i in range(L):\r\n",
        "    if i == l:\r\n",
        "      e.append(1)\r\n",
        "    else:\r\n",
        "      e.append(0)\r\n",
        "  e_y = np.asarray(e)\r\n",
        "  return e_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6UpazPvcV6c"
      },
      "source": [
        "def deriv_activ(z, activation):\r\n",
        "  if(activation == \"relu\"):\r\n",
        "    z[z<=0] = 0\r\n",
        "    z[z>0] = 1\r\n",
        "    return z\r\n",
        "\r\n",
        "  elif(activation == \"sigmoid\"):\r\n",
        "    g_deriv = sigmoid(z) * (1 - sigmoid(z))\r\n",
        "    return g_deriv\r\n",
        "  \r\n",
        "  elif(activation == \"tanh\"):\r\n",
        "    deriv = 1 - (tanh(z)) ** 2\r\n",
        "    return deriv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGp9a-HAVJaZ"
      },
      "source": [
        "def back_propagation(H_caches, A_caches, Wb_matrix, y_hat, y, activation, weight_decay,loss):\r\n",
        "  e_y = []\r\n",
        "  L = int((len(Wb_matrix)/2))\r\n",
        "  num_train = y_hat.shape[1]\r\n",
        "  num_classes = y_hat.shape[0]\r\n",
        "  for j in range(num_train):\r\n",
        "    e_y.append(one_hot_encoding(y[j],num_classes))\r\n",
        "  e_y = np.reshape(e_y,(num_classes,num_train))\r\n",
        "  if loss == \"cross_entropy\":\r\n",
        "    da = -(e_y - y_hat)\r\n",
        "  elif loss == \"mse\":\r\n",
        "    da = (y_hat - e_y)*y_hat - y_hat*(np.dot((y_hat-y).T, y_hat))\r\n",
        "  da = -(e_y - y_hat)\r\n",
        "  grads = {}\r\n",
        "  \r\n",
        "  for k in reversed(range(1,L+1)):\r\n",
        "    dW = np.matmul(da, np.transpose(H_caches[k-1])) + 2*weight_decay*Wb_matrix['W{:d}'.format(k)]\r\n",
        "    db = da\r\n",
        "    if k != 1:\r\n",
        "      dh = np.dot(Wb_matrix['W{:d}'.format(k)].T, da)\r\n",
        "      da = np.multiply(dh, deriv_activ(A_caches[k-2], activation))\r\n",
        "    grads[\"dW\" + str(k)] = dW\r\n",
        "    grads[\"db\" + str(k)] = db\r\n",
        "  \r\n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l16UXHD_YsCH"
      },
      "source": [
        "def sgd(Wb_matrix, grads, eta,L):\r\n",
        "  for l in range(L):\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - eta * grads[\"dW\" + str(l+1)]\r\n",
        "    Wb_matrix['b' + str(l+1)] = Wb_matrix['b' + str(l+1)] - eta * grads[\"db\" + str(l+1)]\r\n",
        "  return Wb_matrix\r\n",
        "\r\n",
        "def momentum(Wb_matrix, grads, eta, gamma, update,L):\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = (gamma * update['W' + str(l+1)]) + eta * grads['dW' + str(l+1)]\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - update['W' + str(l+1)]\r\n",
        "\r\n",
        "    update[\"b\" + str(l+1)] = (gamma * update[\"b\" + str(l+1)]) + eta * grads[\"db\" + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - update[\"b\" + str(l+1)]\r\n",
        "  \r\n",
        "  return Wb_matrix, update\r\n",
        "\r\n",
        "def nag(Wb_matrix,eta,gamma, update, L):\r\n",
        "  Wb_matrix_temp= {}\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = (gamma * update['W' + str(l+1)])\r\n",
        "    Wb_matrix_temp['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - update['W' + str(l+1)]\r\n",
        "\r\n",
        "    update[\"b\" + str(l+1)] = (gamma * update[\"b\" + str(l+1)])\r\n",
        "    Wb_matrix_temp[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - update[\"b\" + str(l+1)]\r\n",
        "\r\n",
        "  return Wb_matrix_temp, update\r\n",
        "\r\n",
        "def rmsprop(Wb_matrix, eta, update ,beta1, eps, grads, L):\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = beta1 * update['W' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]**2\r\n",
        "    update[\"b\" + str(l+1)] = beta1 * update[\"b\" + str(l+1)] + (1-beta1) * grads[\"db\" + str(l+1)]**2\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - (eta/np.sqrt(update['W' + str(l+1)]+eps)) * grads['dW' + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - (eta/np.sqrt(update['b' + str(l+1)]+eps)) * grads['db' + str(l+1)]\r\n",
        "  return Wb_matrix, update\r\n",
        "  \r\n",
        "def adam(Wb_matrix, eta, beta1, beta2, update, v_n, eps, grads, L,i):\r\n",
        "  import math\r\n",
        "  m_hat = update.copy()\r\n",
        "  v_hat = update.copy()\r\n",
        "  for l in range(L):\r\n",
        "    update['W' + str(l+1)] = beta1 * update['W' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]\r\n",
        "    update['b' + str(l+1)] = beta1 * update['b' + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\r\n",
        "    \r\n",
        "    v_n['W' + str(l+1)] = beta2 * v_n['W' + str(l+1)] + (1-beta2) * grads['dW' + str(l+1)]**2\r\n",
        "    v_n[\"b\" + str(l+1)] = beta2 * v_n[\"b\" + str(l+1)] + (1-beta2) * grads['db' + str(l+1)]**2\r\n",
        "    \r\n",
        "    m_hat['W' + str(l+1)] = update['W' + str(l+1)]/(1-math.pow(beta1,i+1))\r\n",
        "    m_hat['b' + str(l+1)] = update['b' + str(l+1)]/(1-math.pow(beta1,i+1))\r\n",
        "\r\n",
        "    v_hat['W' + str(l+1)] = v_n['W' + str(l+1)]/(1-math.pow(beta2,i+1))\r\n",
        "    v_hat['b' + str(l+1)] = v_n['b' + str(l+1)]/(1-math.pow(beta2,i+1))\r\n",
        "\r\n",
        "\r\n",
        "    Wb_matrix['W' + str(l+1)] = Wb_matrix['W' + str(l+1)] - (eta/(np.sqrt(v_hat['W' + str(l+1)]+eps)))* m_hat['W' + str(l+1)]\r\n",
        "    Wb_matrix[\"b\" + str(l+1)] = Wb_matrix[\"b\" + str(l+1)] - (eta/(np.sqrt(v_hat['b' + str(l+1)]+eps)))* m_hat['b' + str(l+1)]\r\n",
        "\r\n",
        " \r\n",
        "  return Wb_matrix, update, v_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe0gM5BCCV8p"
      },
      "source": [
        "# Question 4 (10 Marks)\r\n",
        "\r\n",
        "Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()).  Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.\r\n",
        "\r\n",
        "- number of epochs: 5, 10\r\n",
        "- number of hidden layers:  3, 4, 5\r\n",
        "- size of every hidden layer:  32, 64, 128\r\n",
        "- weight decay (L2 regularisation): 0, 0.0005,  0.5\r\n",
        "- learning rate: 1e-3, 1 e-4 \r\n",
        "- optimizer:  sgd, momentum, nesterov, rmsprop, adam, nadam\r\n",
        "- batch size: 16, 32, 64\r\n",
        "- weight initialisation: random, Xavier\r\n",
        "- activation functions: sigmoid, tanh, ReLU\r\n",
        "\r\n",
        "wandb will automatically generate the following plots. Paste these plots below using the \"Add Panel to Report\" feature. Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU) instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo0kRUHIQMDO"
      },
      "source": [
        "def swp():\r\n",
        "  from keras.datasets import fashion_mnist\r\n",
        "  from matplotlib import pyplot\r\n",
        "  import numpy as np\r\n",
        "  import math\r\n",
        "  import random\r\n",
        "  from sklearn.metrics import confusion_matrix\r\n",
        "  import seaborn as sns\r\n",
        "  from sklearn.model_selection import train_test_split\r\n",
        "  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n",
        "  X_train_flatten = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))/255.0\r\n",
        "  \r\n",
        "  X_test_flatten = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))/255.0\r\n",
        "  X_test_flatten = X_test_flatten.T\r\n",
        "\r\n",
        "  X_train_flatten, X_val, y_train, y_val = train_test_split( X_train_flatten, y_train, test_size=0.1, random_state=42)\r\n",
        "  X_train_flatten = X_train_flatten.T\r\n",
        "  X_val = X_val.T\r\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\n",
        "  num_train = int(0.9*X_train.shape[0])\r\n",
        "  num_val = int(0.1*X_train.shape[0])\r\n",
        " \r\n",
        "  hyperparameter_defaults = dict(\r\n",
        "      num_hidden = 5,\r\n",
        "      size_hidden = 128,\r\n",
        "      weight_decay = 0,\r\n",
        "      optimizer = 'adam',\r\n",
        "      initializer = 'Xavier',\r\n",
        "      activation = 'relu',\r\n",
        "      batch_size = 64,\r\n",
        "      learning_rate = 0.001,\r\n",
        "      max_epoch = 10,\r\n",
        "      loss_func = 'cross_entropy'\r\n",
        "      )\r\n",
        "\r\n",
        "  \r\n",
        "  wandb.init(project=\"cs6910-assignment01\", config=hyperparameter_defaults)\r\n",
        "  config = wandb.config\r\n",
        "  wandb.run.name = \"hl{}_bs_{}_ac_{}\".format(config.num_hidden,config.batch_size, config.activation)\r\n",
        "  L = config.num_hidden+1\r\n",
        "  beta1 = 0.9\r\n",
        "  beta2 = 0.999\r\n",
        "  eps = 1e-8\r\n",
        "  gamma = 0.9\r\n",
        "\r\n",
        "\r\n",
        "  Wb_matrix, update, grad_initial = initialize_Wb_matrix(X_train_flatten, config.num_hidden , config.size_hidden ,config.initializer)\r\n",
        "  #initializing parameters in adam optimization to zero dictionary\r\n",
        "  v = update.copy()\r\n",
        "  m_hat = update.copy()\r\n",
        "  v_hat = update.copy()\r\n",
        "\r\n",
        "  \r\n",
        "  for i in range(config.max_epoch):\r\n",
        "    loss = 0\r\n",
        "    val_loss = 0\r\n",
        "    count = 0\r\n",
        "    grads = grad_initial\r\n",
        "    print(\"Epoch:\", i+1)\r\n",
        "    for j in range(num_train):\r\n",
        "      train_ip= np.reshape(X_train_flatten[:,j], (-1, 1)) \r\n",
        "      y_hat, H_caches, A_caches = forward_propagation(train_ip, Wb_matrix ,config.activation)\r\n",
        "      if(y_train[j]==np.argmax(y_hat)):\r\n",
        "        count = count + 1; \r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0  :\r\n",
        "        if config.optimizer == 'nag':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "\r\n",
        "      grad = back_propagation(H_caches, A_caches, Wb_matrix, y_hat, [y_train[j]], config.activation, config.weight_decay,config.loss_func)\r\n",
        "      for l in range(L):\r\n",
        "        grads['dW' + str(l+1)] = grads['dW' + str(l+1)] + grad['dW' + str(l+1)] \r\n",
        "        grads['db' + str(l+1)] = grads['db' + str(l+1)] + grad['db' + str(l+1)] \r\n",
        "\r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0 : \r\n",
        "        for l in range(L):\r\n",
        "          grads['dW' + str(l+1)] = grads['dW' + str(l+1)]/config.batch_size\r\n",
        "          grads['db' + str(l+1)] = grads['db' + str(l+1)]/config.batch_size\r\n",
        "        if config.optimizer == 'sgd':\r\n",
        "          Wb_matrix = sgd(Wb_matrix, grads, config.learning_rate,L)\r\n",
        "        elif config.optimizer == 'momentum':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'nag':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'rmsprop':\r\n",
        "          Wb_matrix, update = rmsprop(Wb_matrix, config.learning_rate, update,  beta1, eps, grads, L)\r\n",
        "        elif config.optimizer == 'adam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)    \r\n",
        "      if config.loss_func == 'cross_entropy':\r\n",
        "        loss = loss - (1/num_train)*math.log(y_hat[y_train[j]])\r\n",
        "      elif config.loss_func == 'mse':\r\n",
        "        loss = loss + (1/num_train)*(max(y_hat) - y_train[j])**2\r\n",
        "    \r\n",
        "    y_hat_val, H_caches_val, A_caches_val = forward_propagation(X_val, Wb_matrix ,config.activation)\r\n",
        "    count_val = np.sum(np.argmax(y_hat_val, axis = 0)== y_val)\r\n",
        "    \r\n",
        "    for j in range(num_val):\r\n",
        "      val_ip= np.reshape(X_val[:,j], (-1, 1)) \r\n",
        "      y_hat_val, H_caches_val, A_caches_val = forward_propagation(val_ip, Wb_matrix ,config.activation)\r\n",
        "      if config.loss_func == 'cross_entropy':\r\n",
        "        val_loss = val_loss - (1/num_val)*math.log(y_hat_val[int(y_val[j])])\r\n",
        "      elif config.loss_func == 'mse':\r\n",
        "        val_loss = val_loss + (1/num_val)*(max(y_hat_val) - y_val[j])**2\r\n",
        "\r\n",
        "    accuracy = 100*count/num_train\r\n",
        "    val_accuracy = 100*count_val/num_val\r\n",
        "\r\n",
        "    print(\"     Loss\", loss)\r\n",
        "    print(\"     Accuracy\",accuracy)\r\n",
        "    print(\"     Validation Loss\", val_loss)\r\n",
        "    print(\"     Validation Accuracy\", val_accuracy)\r\n",
        "\r\n",
        "    metrics = {'epoch':i, 'val_accuracy': val_accuracy, 'val_loss': val_loss, 'accuracy': accuracy, 'loss': loss}\r\n",
        "    wandb.log(metrics)\r\n",
        "\r\n",
        "  y_hat_test, H_caches_test, A_caches_test = forward_propagation(X_test_flatten, Wb_matrix ,config.activation)\r\n",
        "  wandb.log({\"Confusion_Matrix\" : wandb.plot.confusion_matrix(\r\n",
        "                        probs=None,\r\n",
        "                        y_true=y_test,\r\n",
        "                        preds=np.argmax(y_hat_test, axis = 0),\r\n",
        "                        class_names=class_names)})\r\n",
        "  \r\n",
        "\r\n",
        "  wandb.run.finish()\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfKEScFrjZVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d77dd5-3ff1-4b2f-972b-2c4b5f111cc6"
      },
      "source": [
        "import wandb\r\n",
        "\r\n",
        "sweep_config = {\r\n",
        "  \"name\": \"My Sweep\",\r\n",
        "  \"method\": \"grid\",\r\n",
        "  \"project\": \"cs6910-assignment01\",\r\n",
        "  \"metric\":{\r\n",
        "      \"name\":\"val_accuracy\",\r\n",
        "      \"goal\":\"maximize\"\r\n",
        "  },\r\n",
        "  \"parameters\": {\r\n",
        "        \"max_epoch\": {\r\n",
        "            \"values\": [10]\r\n",
        "        },\r\n",
        "        \"num_hidden\": {\r\n",
        "            \"values\":[5]\r\n",
        "        }, \r\n",
        "        \"size_hidden\": {\r\n",
        "            \"values\":[128]\r\n",
        "        },\r\n",
        "        \"weight_decay\":{\r\n",
        "            \"values\":[0.0005]\r\n",
        "        },\r\n",
        "        \"learning_rate\":{\r\n",
        "            \"values\":[0.001]\r\n",
        "        },\r\n",
        "        \"batch_size\": {\r\n",
        "            \"values\":[16]\r\n",
        "        },  \r\n",
        "        \"optimizer\": {\r\n",
        "            \"values\":['rmsprop', 'adam', 'nadam']\r\n",
        "        },\r\n",
        "        \"initializer\": {\r\n",
        "            \"values\":['Xavier']\r\n",
        "        },\r\n",
        "        \"activation\":{\r\n",
        "            \"values\": ['relu']\r\n",
        "        },\r\n",
        "        \"loss_func\":{\r\n",
        "            \"values\": ['cross_entropy']\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: jrvbsjg2\n",
            "Sweep URL: https://wandb.ai/karthik21/uncategorized/sweeps/jrvbsjg2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN8v3WbIbw9o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "10a92c0b-924a-49a6-b4e1-16a38027968e"
      },
      "source": [
        "wandb.agent(sweep_id, swp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: skngemgr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_func: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_hidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sandy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karthik21/uncategorized\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karthik21/uncategorized/sweeps/jrvbsjg2\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized/sweeps/jrvbsjg2</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karthik21/uncategorized/runs/skngemgr\" target=\"_blank\">https://wandb.ai/karthik21/uncategorized/runs/skngemgr</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_153315-skngemgr</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "     Loss 0.5697750474974579\n",
            "     Accuracy 79.22222222222223\n",
            "     Validation Loss 0.5344589722873124\n",
            "     Validation Accuracy 80.35\n",
            "Epoch: 2\n",
            "     Loss 0.45790988109401226\n",
            "     Accuracy 83.78703703703704\n",
            "     Validation Loss 0.4777744309036437\n",
            "     Validation Accuracy 83.23333333333333\n",
            "Epoch: 3\n",
            "     Loss 0.4349588823617389\n",
            "     Accuracy 84.76666666666667\n",
            "     Validation Loss 0.46155629908082224\n",
            "     Validation Accuracy 83.7\n",
            "Epoch: 4\n",
            "     Loss 0.4199296449315399\n",
            "     Accuracy 85.23333333333333\n",
            "     Validation Loss 0.4549666638480806\n",
            "     Validation Accuracy 84.6\n",
            "Epoch: 5\n",
            "     Loss 0.41133570723128277\n",
            "     Accuracy 85.47222222222223\n",
            "     Validation Loss 0.47035258755132514\n",
            "     Validation Accuracy 84.36666666666666\n",
            "Epoch: 6\n",
            "     Loss 0.40568975497641635\n",
            "     Accuracy 85.82222222222222\n",
            "     Validation Loss 0.45181976227351045\n",
            "     Validation Accuracy 84.98333333333333\n",
            "Epoch: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXXTVz-EB8iT"
      },
      "source": [
        "import wandb\r\n",
        "\r\n",
        "sweep_config = {\r\n",
        "  \"name\": \"Loss_compare\",\r\n",
        "  \"method\": \"grid\",\r\n",
        "  \"project\": \"cs6910-assignment01\",\r\n",
        "  \"metric\":{\r\n",
        "      \"name\":\"val_accuracy\",\r\n",
        "      \"goal\":\"maximize\"\r\n",
        "  },\r\n",
        "  \"parameters\": {\r\n",
        "        \"max_epoch\": {\r\n",
        "            \"values\":[10]\r\n",
        "        },\r\n",
        "        \"num_hidden\": {\r\n",
        "            \"values\":[5]\r\n",
        "        }, \r\n",
        "        \"size_hidden\": {\r\n",
        "            \"values\":[64]\r\n",
        "        },\r\n",
        "        \"weight_decay\":{\r\n",
        "            \"values\":[0]\r\n",
        "        },\r\n",
        "        \"learning_rate\":{\r\n",
        "            \"values\":[0.001]\r\n",
        "        },\r\n",
        "        \"batch_size\": {\r\n",
        "            \"values\":[16]\r\n",
        "        },  \r\n",
        "        \"optimizer\": {\r\n",
        "            \"values\":['adam']\r\n",
        "        },\r\n",
        "        \"initializer\": {\r\n",
        "            \"values\":['Xavier']\r\n",
        "        },\r\n",
        "        \"activation\":{\r\n",
        "            \"values\": ['relu']\r\n",
        "        },\r\n",
        "        \"loss_func\":{\r\n",
        "            \"values\": ['mse']\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxqHmkN03OBv"
      },
      "source": [
        "# MNIST DATASET\r\n",
        "\r\n",
        "Based on your learnings above, give me 3 recommendations for what would work for the MNIST dataset (not Fashion-MNIST). Just to be clear, I am asking you to take your learnings based on extensive experimentation with one dataset and see if these learnings help on another dataset. If I give you a budget of running only 3 hyperparameter configurations as opposed to the large number of experiments you have run above then which 3 would you use and why. Report the accuracies that you obtain using these 3 configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQi3AJd2MuD"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzjYjqTiH56K"
      },
      "source": [
        "def mnist_swp(hyperparameters):\r\n",
        "  import wandb\r\n",
        "  from keras.datasets import mnist\r\n",
        "  from matplotlib import pyplot\r\n",
        "  import numpy as np\r\n",
        "  import math\r\n",
        "  import random\r\n",
        "  from sklearn.metrics import confusion_matrix\r\n",
        "  import seaborn as sns\r\n",
        "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
        "  X_train_flatten = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))/255.0\r\n",
        "  X_train_flatten = X_train_flatten.T\r\n",
        "  X_test_flatten = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))/255.0\r\n",
        "  X_test_flatten = X_test_flatten.T\r\n",
        "  num_train = X_train.shape[0]\r\n",
        "  num_test = X_test.shape[0]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  loss_func = 'cross_entropy'\r\n",
        "\r\n",
        "  wandb.init(project=\"cs6910-assignment01\", config=hyperparameters)\r\n",
        "  config = wandb.config\r\n",
        "  wandb.run.name = \"MNIST_hl{}_bs_{}_ac_{}\".format(config.num_hidden,config.batch_size, config.activation)\r\n",
        "  L = config.num_hidden+1\r\n",
        "  beta1 = 0.9\r\n",
        "  beta2 = 0.999\r\n",
        "  eps = 1e-8\r\n",
        "  gamma = 0.9\r\n",
        "\r\n",
        "\r\n",
        "  Wb_matrix, update, grad_initial = initialize_Wb_matrix(X_train_flatten, config.num_hidden , config.size_hidden ,config.initializer)\r\n",
        "  v = update.copy()\r\n",
        "  m_hat = update.copy()\r\n",
        "  v_hat = update.copy()\r\n",
        "\r\n",
        "  \r\n",
        "  for i in range(config.max_epoch):\r\n",
        "    loss = 0\r\n",
        "    test_loss = 0\r\n",
        "    count = 0\r\n",
        "    grads = grad_initial\r\n",
        "    print(\"Epoch:\", i+1)\r\n",
        "    for j in range(num_train):\r\n",
        "      train_ip= np.reshape(X_train_flatten[:,j], (-1, 1)) \r\n",
        "      y_hat, H_caches, A_caches = forward_propagation(train_ip, Wb_matrix ,config.activation)\r\n",
        "      if(y_train[j]==np.argmax(y_hat)):\r\n",
        "        count = count + 1; \r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0  :\r\n",
        "        if config.optimizer == 'nag':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update = nag(Wb_matrix, config.learning_rate,gamma, update, L)\r\n",
        "\r\n",
        "      grad = back_propagation(H_caches, A_caches, Wb_matrix, y_hat, [y_train[j]], config.activation, config.weight_decay,loss_func)\r\n",
        "      for l in range(L):\r\n",
        "        grads['dW' + str(l+1)] = grads['dW' + str(l+1)] + grad['dW' + str(l+1)] \r\n",
        "        grads['db' + str(l+1)] = grads['db' + str(l+1)] + grad['db' + str(l+1)] \r\n",
        "\r\n",
        "\r\n",
        "      if (j+1)%config.batch_size == 0 : \r\n",
        "        for l in range(L):\r\n",
        "          grads['dW' + str(l+1)] = grads['dW' + str(l+1)]/config.batch_size\r\n",
        "          grads['db' + str(l+1)] = grads['db' + str(l+1)]/config.batch_size\r\n",
        "        if config.optimizer == 'sgd':\r\n",
        "          Wb_matrix = sgd(Wb_matrix, grads, config.learning_rate,L)\r\n",
        "        elif config.optimizer == 'momentum':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'nag':\r\n",
        "          Wb_matrix,update = momentum(Wb_matrix,grads,config.learning_rate,gamma,update,L)\r\n",
        "        elif config.optimizer == 'rmsprop':\r\n",
        "          Wb_matrix, update = rmsprop(Wb_matrix, config.learning_rate, update,  beta1, eps, grads, L)\r\n",
        "        elif config.optimizer == 'adam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)\r\n",
        "        elif config.optimizer == 'nadam':\r\n",
        "          Wb_matrix, update,v = adam(Wb_matrix, config.learning_rate, beta1, beta2, update, v, eps, grads, L, i+1)    \r\n",
        "      \r\n",
        "      if loss_func == 'cross_entropy':\r\n",
        "        loss = loss - (1/num_train)*math.log(y_hat[y_train[j]])\r\n",
        "      elif loss_func == 'mse':\r\n",
        "        loss = loss + (1/num_train)*(max(y_hat) - y_train[j])\r\n",
        "    \r\n",
        "    y_hat_test, H_caches_test, A_caches_test = forward_propagation(X_test, Wb_matrix ,config.activation)\r\n",
        "    count_test = np.sum(np.argmax(y_hat_test, axis = 0)== y_test)\r\n",
        "    \r\n",
        "    for j in range(num_test):\r\n",
        "      test_ip= np.reshape(X_test[:,j], (-1, 1)) \r\n",
        "      y_hat_test, H_caches_test, A_caches_test = forward_propagation(test_ip, Wb_matrix ,config.activation)\r\n",
        "      if loss_func == 'cross_entropy':\r\n",
        "        test_loss = test_loss - (1/num_test)*math.log(y_hat_test[int(y_test[j])])\r\n",
        "      elif loss_func == 'mse':\r\n",
        "        test_loss = test_loss + (1/num_test)*(max(y_hat_test) - y_test[j])**2\r\n",
        "    accuracy = 100*count/num_train\r\n",
        "    test_accuracy = 100*count_test/num_test\r\n",
        "\r\n",
        "    print(\"     Loss\", loss)\r\n",
        "    print(\"     Accuracy\",accuracy)\r\n",
        "    print(\"     Test Loss\", test_loss)\r\n",
        "    print(\"     Test Accuracy\", test_accuracy)\r\n",
        "\r\n",
        "    metrics = {'epoch':i, 'test_accuracy': test_accuracy, 'test_loss': test_loss, 'training_accuracy': accuracy, 'training_loss': loss}\r\n",
        "    wandb.log(metrics)\r\n",
        "\r\n",
        "  wandb.run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bnjeNJDy1lm"
      },
      "source": [
        "hyperparameters1 = dict(\r\n",
        "    num_hidden = 5,\r\n",
        "    size_hidden = 128,\r\n",
        "    weight_decay = 0,\r\n",
        "    optimizer = 'adam',\r\n",
        "    initializer = 'Xavier',\r\n",
        "    activation = 'relu',\r\n",
        "    batch_size = 64,\r\n",
        "    learning_rate = 0.001,\r\n",
        "    max_epoch = 10,\r\n",
        "    )\r\n",
        "mnist_swp(hyperparameters1)\r\n",
        "\r\n",
        "hyperparameters2 = dict(\r\n",
        "    num_hidden = 5,\r\n",
        "    size_hidden = 64,\r\n",
        "    weight_decay = 0,\r\n",
        "    optimizer = 'nag',\r\n",
        "    initializer = 'Xavier',\r\n",
        "    activation = 'relu',\r\n",
        "    batch_size = 16,\r\n",
        "    learning_rate = 0.001,\r\n",
        "    max_epoch = 10,\r\n",
        "    )\r\n",
        "\r\n",
        "mnist_swp(hyperparameters2)\r\n",
        "\r\n",
        "hyperparameters3 = dict(\r\n",
        "    num_hidden = 5,\r\n",
        "    size_hidden = 128,\r\n",
        "    weight_decay = 0.0005,\r\n",
        "    optimizer = 'adam',\r\n",
        "    initializer = 'Xavier',\r\n",
        "    activation = 'relu',\r\n",
        "    batch_size = 16,\r\n",
        "    learning_rate = 0.000e1,\r\n",
        "    max_epoch = 10,\r\n",
        "    )\r\n",
        "\r\n",
        "mnist_swp(hyperparameters3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "camj1lLku_K2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab543306-1ed4-4f72-f960-01a0c52f7806"
      },
      "source": [
        "import wandb\r\n",
        "\r\n",
        "sweep_config = {\r\n",
        "  \"name\": \"MNIST sweep 2\",\r\n",
        "  \"method\": \"random\",\r\n",
        "  \"project\": \"cs6910-assignment01_fullsweep\",\r\n",
        "  \"metric\":{\r\n",
        "      \"name\":\"test_accuracy\",\r\n",
        "      \"goal\":\"maximize\"\r\n",
        "  },\r\n",
        "  \"parameters\": {\r\n",
        "        \"max_epoch\": {\r\n",
        "            \"values\": [10]\r\n",
        "        },\r\n",
        "        \"num_hidden\": {\r\n",
        "            \"values\":[5]\r\n",
        "        }, \r\n",
        "        \"size_hidden\": {\r\n",
        "            \"values\":[128]\r\n",
        "        },\r\n",
        "        \"weight_decay\":{\r\n",
        "            \"values\":[0]\r\n",
        "        },\r\n",
        "        \"learning_rate\":{\r\n",
        "            \"values\":[0.001]\r\n",
        "        },\r\n",
        "        \"batch_size\": {\r\n",
        "            \"values\":[16]\r\n",
        "        },  \r\n",
        "        \"optimizer\": {\r\n",
        "            \"values\":['nag']\r\n",
        "        },\r\n",
        "        \"initializer\": {\r\n",
        "            \"values\":['Xavier']\r\n",
        "        },\r\n",
        "        \"activation\":{\r\n",
        "            \"values\": ['relu']\r\n",
        "        },\r\n",
        "        \"loss_func\":{\r\n",
        "            \"values\": ['cross_entropy']\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: g1j26dbr\n",
            "Sweep URL: https://wandb.ai/karthik21/uncategorized/sweeps/g1j26dbr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXINphelC-9O"
      },
      "source": [
        ""
      ]
    }
  ]
}