{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_PartB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodikarthik21/CS6910---Fundamentals-of-Deep-Learning/blob/main/Assignment02/Assignment2_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bprvPijtrqfr"
      },
      "source": [
        "# INSTALLING REQUIRED DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WuiCFS-bK1e"
      },
      "source": [
        "#importing Google drive to load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cRsfdaKdXn"
      },
      "source": [
        "#installing wandb\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUxYr82I0HsZ"
      },
      "source": [
        "#unzipping the dataset which is stored in the Drive\n",
        "!unzip '/content/drive/MyDrive/nature_12K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaHsw9pmHilA"
      },
      "source": [
        "# IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Zs2zfiHniB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.applications import InceptionV3, InceptionResNetV2, ResNet50, Xception"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUMI0vextQ7K"
      },
      "source": [
        "# SPLITTING THE TRAINING AND VALIDATION SETS\n",
        "As in the previous part, we use image_dataset_from_directory module to split the training and the validation datasets. Here, 90% of the training dataset is used for training and 10% is used for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biJEMvj8Jtxd",
        "outputId": "ed2482c4-c5f9-4013-e9f8-b96002b84b73"
      },
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory('/content/inaturalist_12K/train', \n",
        "                                                                    validation_split=0.1, seed = 123, subset = \"training\")\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory('/content/inaturalist_12K/train', \n",
        "                                                                    validation_split=0.1, seed = 123, subset = \"validation\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9999 files belonging to 10 classes.\n",
            "Using 9000 files for training.\n",
            "Found 9999 files belonging to 10 classes.\n",
            "Using 999 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlYf7mkRtuOP"
      },
      "source": [
        "# DATA AUGMENTATION\n",
        "Four augmentations, viz. Random Flip, Random Crop, Random Rotation, Random Translation are being used here. These are chosen because these are the possible modifications which the model will see during the test time and thus, it is important that the model recognises these modified images correctly as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEzZwYv2J5t-"
      },
      "source": [
        "IMG_SIZE = 256\n",
        "\n",
        "data_aug = tf.keras.Sequential([preprocessing.RandomFlip(), #Random Flip\n",
        "                                preprocessing.RandomCrop(IMG_SIZE, IMG_SIZE), #Random Crop\n",
        "                                preprocessing.RandomRotation(factor = (-0.2, 0.2)), #Random Rotation\n",
        "                                preprocessing.RandomTranslation(height_factor=(-0.2, 0.2), width_factor=(-0.2,0.2)), #Random Translation\n",
        "                              ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKXc8YvYz9fA"
      },
      "source": [
        "# DEFINING THE MODEL\n",
        "First, I am running a dummy example as a template for the final code. As an illustration, I'm using InceptionV3 as the base model. Apart from the base model, I have added an Average Pooling Layer and a softmax output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtr50SDIajv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ea4d79-fabc-49f0-f194-165861a40e3c"
      },
      "source": [
        "preprocess_input = tf.keras.applications.inception_v3.preprocess_input #the preprocessing input function for InceptionV3\n",
        "base_model = InceptionV3(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3)) #defining the base model\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()  #average pooling layer\n",
        "prediction_layer = tf.keras.layers.Dense(10, activation=\"softmax\")  #final softmax layer"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHCQ68z7LxuF"
      },
      "source": [
        "#Model Definition\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = data_aug(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pNWlz7R1rvC"
      },
      "source": [
        "# EXPERIMENT-1\n",
        "Base Model: InceptionV3 <br>\n",
        "Learning rate = 1e-3 <br>\n",
        "Optimizer: Adam <br>\n",
        "Freezing 275/311 layers <br>\n",
        "Number of epochs: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ND5_kFNjVo"
      },
      "source": [
        "#Defining the hyperparameters such as optimizer and learning rate\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "learning_rate = 1e-3\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics='accuracy')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iybpV6oNP3G3"
      },
      "source": [
        "#Number of layers in the base model so that we can freeze a certain number of layers appropriately\n",
        "base_model.trainable = True\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCVgjcAlRcut"
      },
      "source": [
        "#Freezing 275/311 layers\n",
        "fine_tune_at = 275\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNvXb6jyRvZ5"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "model.fit(train_dataset, validation_data = validation_dataset, epochs = epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C163Xq2k2rAG"
      },
      "source": [
        "# EXPERIMENT-2\n",
        "Base Model: InceptionV3 <br>\n",
        "Learning rate = 1e-3 <br>\n",
        "Optimizer: Adam <br>\n",
        "Freezing 300/311 layers <br>\n",
        "Number of epochs: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPc40whNRzAd"
      },
      "source": [
        "#defining a function so that it would be easier to run more experiments\n",
        "def pre_train_adam(fine_tune_at):\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  learning_rate = 1e-3\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics='accuracy')\n",
        "  epochs = 10\n",
        "  model.fit(train_dataset, validation_data = validation_dataset, epochs = epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ7YwprSWOXB"
      },
      "source": [
        "pre_train_adam(300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bhiNzYy3Kzm"
      },
      "source": [
        "# EXPERIMENT-3\n",
        "Base Model: InceptionV3 <br>\n",
        "Learning rate = 1e-3 <br>\n",
        "Optimizer: NAG <br>\n",
        "Freezing 300/311 layers <br>\n",
        "Number of epochs: 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu9vJEXIWTf5"
      },
      "source": [
        "#defining a similar function\n",
        "def pre_train_nag(fine_tune_at, learning_rate=1e-3, epochs=10):\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate, decay=learning_rate/epochs, nesterov=True),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics='accuracy')\n",
        "  \n",
        "  model.fit(train_dataset, validation_data = validation_dataset, epochs = epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8UCE78Ynm_5"
      },
      "source": [
        "pre_train_nag(300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuV4Brty3WNW"
      },
      "source": [
        "# WANDB SWEEP FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaTwHzXyBL4y"
      },
      "source": [
        "def swp():\n",
        "\"\"\"\n",
        "  List of various hyperparameters and their values:\n",
        "  base: base model, string, \"IncV3\" for InceptionV3, \"IncResV2\" for InceptionResNetV2, \"Res50\" for ResNet50 and \"Xcep\" for Xception\n",
        "  fine_tune_at_fraction: fraction of total number of  layers to be frozen, chosen among 1,0.95,0.9,0.85,0.8\n",
        "  epochs: Total number of epochs, chosen among 5,10,15\n",
        "  data_augmentation: whether data augmentation has to be included or not, \"Y\" if it has to be included, \"N\" if it need not be included\n",
        "  dropout: fraction of nodes for dropout, chosen among 0,0.2,0.3,0.4\n",
        "  optimizer: optimizer to be used for training, \"adam\" for Adam optimizer, \"nadam\" for Nadam optimizer, \"rmsprop\" for RMSProp optimizer and \"nag\" for NAG optimizer.\n",
        "\n",
        "\"\"\"\n",
        "#---------DEFAULT HYPERPARAMETERS-----------------------------------------------\n",
        "  hyperparameter_defaults = dict(\n",
        "        base = \"IncV3\",\n",
        "        fine_tune_at_fraction = 0.9,\n",
        "        optimizer = 'nag',\n",
        "        epochs = 10,\n",
        "        data_augmentation = \"Y\",\n",
        "        dropout = 0.2\n",
        "      )\n",
        "\n",
        "#----------------SETTING UP WANDB-----------------------------------------------\n",
        "  wandb.init(project=\"Assignment 2\", config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "  wandb.run.name = \"{}_base_{}_finetune_{}_opt_{}_epoch_{}_DataAug_{}_dropout\".format(config.base, config.fine_tune_at_fraction,config.optimizer, config.epochs, config.data_augmentation, config.dropout)\n",
        "\n",
        "#----------------BASE MODEL DEFINITION AND INPUT PREPROCESSING------------------  \n",
        "  if config.base == \"IncV3\": #InceptionV3\n",
        "    preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
        "    base_model = InceptionV3(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "  elif config.base == \"IncResV2\": #InceptionResNetV2\n",
        "    preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
        "    base_model = InceptionResNetV2(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "  \n",
        "  elif config.base == \"Res50\": #ResNet50\n",
        "    preprocess_input = tf.keras.applications.resnet.preprocess_input\n",
        "    base_model = ResNet50(include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
        "\n",
        "  elif config.base == \"Xcep\": #Xception\n",
        "    preprocess_input = tf.keras.applications.xception.preprocess_input\n",
        "    base_model = Xception(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "#---------------DEFINING THE REST OF THE MODEL----------------------------------\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()  #Average pooling layer\n",
        "  prediction_layer = tf.keras.layers.Dense(10, activation=\"softmax\") #softmax output layer\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "  if config.data_augmentation == \"Y\": #including data augmentation\n",
        "    x = data_aug(inputs) \n",
        "    x = preprocess_input(x)\n",
        "    x = base_model(x)\n",
        "    x = global_average_layer(x)\n",
        "    x = tf.keras.layers.Dropout(config.dropout)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "  else: #without data augmentation\n",
        "    x = preprocess_input(inputs)\n",
        "    x = base_model(x)\n",
        "    x = global_average_layer(x)\n",
        "    x = tf.keras.layers.Dropout(config.dropout)(x)\n",
        "    outputs = prediction_layer(x)\n",
        "\n",
        "#------------------FINE TUNING--------------------------------------------------\n",
        "  fine_tune_at = int(len(base_model.layers) * config.fine_tune_at_fraction) #finding the number of layers to be 'frozen'\n",
        "\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False  \n",
        "\n",
        "#------------------OPTIMIZERS---------------------------------------------------\n",
        "  learning_rate = 1e-3\n",
        "  decay_rate = learning_rate / config.epochs\n",
        "\n",
        "  if config.optimizer == 'adam': #Adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'nadam': #Nadam optimizer\n",
        "    optimizer = tf.keras.optimizers.Nadam(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'rmsprop': #RMSProp optimizer\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay_rate)\n",
        "  elif config.optimizer == 'nag': #NAG optimizer\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate, decay=decay_rate, nesterov=True)\n",
        "\n",
        "#-----------------DEFINING THE MODEL--------------------------------------------  \n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(train_dataset, validation_data = validation_dataset, epochs = config.epochs, callbacks = [WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGLjAp5lHvXI"
      },
      "source": [
        "import wandb\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"My Sweep\",\n",
        "  \"method\": \"random\",\n",
        "  \"project\": \"Assignment02\",\n",
        "  \"metric\":{\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\":\"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"base\": {\n",
        "            \"values\": ['IncV3', 'IncResV2', 'Res50', 'Xcep']\n",
        "        },\n",
        "        \"fine_tune_at_fraction\": {\n",
        "            \"values\":[1, 0.95, 0.9, 0.85, 0.8]\n",
        "        }, \n",
        "        \"epochs\": {\n",
        "            \"values\":[5,10,15]\n",
        "        },\n",
        "        \"data_augmentation\":{\n",
        "            \"values\":['Y',\"N\"]\n",
        "        },\n",
        "        \"dropout\":{\n",
        "            \"values\":[0,0.2,0.3,0.4]\n",
        "        },  \n",
        "        \"optimizer\": {\n",
        "            \"values\":['adam', 'nadam','rmsprop','nag']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyAz51i4WFd6"
      },
      "source": [
        "#wandb.agent(sweep_id, function=swp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}