# Assignment 3 - Sequence to Sequence model for Machine Transliteration

Links: [Wandb Report](https://wandb.ai/kodikarthik21/Assignment-3/reports/Assignment-3-Recurrent-Neural-Networks--Vmlldzo2MzczNjY), [seq2seq_without_attention Colab link](https://colab.research.google.com/drive/1a_Tlb7zOLIna7lk0dbFfWBQIBRPZDsQz?usp=sharing), [seq2seq_with_attention Colab link](https://colab.research.google.com/drive/17YmC8Lyj3ZDgMXzVVtG_4vrCed7mypsg?usp=sharing)

## Preliminaries
Install wandb and store the dataset of your preferred language from [here](https://github.com/google-research-datasets/dakshina) in your Drive with the name lexicons.zip. Run the code under "Attention Class" in [seq_2_seq with attention](https://colab.research.google.com/drive/17YmC8Lyj3ZDgMXzVVtG_4vrCed7mypsg?usp=sharing).

## Dataset preprocessing
Run the cells under the heading of "DATASET PREPROCESSING" to convert the train, validation and test datasets to suitable matrix representations. 
