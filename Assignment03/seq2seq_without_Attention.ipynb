{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq_without_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t9oJwzED1aTX",
        "QdzFgPJ41jaV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodikarthik21/CS6910---Fundamentals-of-Deep-Learning/blob/main/Assignment03/seq2seq_without_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt5gtWQN1KV3"
      },
      "source": [
        "# IMPORTING DATASET AND INSTALLING REQUIRED DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXMvklWP41yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae15cfee-4f51-40cf-aa7a-e0cf3de80e94"
      },
      "source": [
        "#Importing dataset from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4BZBkwtYydm",
        "outputId": "95182a87-46db-4743-8d89-b09ead50e945"
      },
      "source": [
        "#getting the dataset\n",
        "!unzip '/content/drive/MyDrive/lexicons.zip'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/lexicons.zip\n",
            "replace lexicons/ta.translit.sampled.test.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqn-Ah4BVhLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7478c6d3-05cd-4fa3-e2cf-b633db25ae1e"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.30)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (8.0.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.1.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.17)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ2dlNYDASNE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a88835-cd47-4b41-c98e-2ba7d3e29b79"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkodikarthik21\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oJwzED1aTX"
      },
      "source": [
        "# IMPORTING NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv-BjJkmtJfD"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from wandb.keras import WandbCallback\n",
        "from math import log"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdzFgPJ41jaV"
      },
      "source": [
        "# DATASET PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFeaNePLZuvV"
      },
      "source": [
        "#converting tsv file to pandas dataframe for easier processing\n",
        "train_df = pd.read_csv(\"/content/lexicons/ta.translit.sampled.train.tsv\", sep = '\\t', header = None)\n",
        "train_df = train_df.dropna(axis=0)\n",
        "\n",
        "val_df = pd.read_csv(\"/content/lexicons/ta.translit.sampled.dev.tsv\", sep = '\\t', header = None)\n",
        "val_df = val_df.dropna(axis=0)\n",
        "\n",
        "test_df = pd.read_csv(\"/content/lexicons/ta.translit.sampled.test.tsv\", sep = '\\t', header = None)\n",
        "test_df = test_df.dropna(axis=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-icrnCDBdyn8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "979629ca-4bc1-41e1-ae13-c9dac904d2fa"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ஃபியட்</td>\n",
              "      <td>fiat</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ஃபியட்</td>\n",
              "      <td>phiyat</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ஃபியட்</td>\n",
              "      <td>piyat</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ஃபிரான்ஸ்</td>\n",
              "      <td>firaans</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ஃபிரான்ஸ்</td>\n",
              "      <td>france</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68213</th>\n",
              "      <td>ஹோல்ட்</td>\n",
              "      <td>holtt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68214</th>\n",
              "      <td>ஹோல்ட்</td>\n",
              "      <td>hoold</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68215</th>\n",
              "      <td>ஹோல்ட்</td>\n",
              "      <td>hoolt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68216</th>\n",
              "      <td>ஹோல்ட்</td>\n",
              "      <td>hooltt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68217</th>\n",
              "      <td>ஹோல்ட்</td>\n",
              "      <td>hult</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68215 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0        1  2\n",
              "0         ஃபியட்     fiat  2\n",
              "1         ஃபியட்   phiyat  1\n",
              "2         ஃபியட்    piyat  1\n",
              "3      ஃபிரான்ஸ்  firaans  1\n",
              "4      ஃபிரான்ஸ்   france  2\n",
              "...          ...      ... ..\n",
              "68213     ஹோல்ட்    holtt  1\n",
              "68214     ஹோல்ட்    hoold  1\n",
              "68215     ஹோல்ட்    hoolt  1\n",
              "68216     ஹோல்ட்   hooltt  1\n",
              "68217     ஹோல்ட்     hult  1\n",
              "\n",
              "[68215 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOrfl7iJRQ4r"
      },
      "source": [
        "## Train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xenfuPMts4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809623ce-c8b0-4afb-ddb8-2147848123b2"
      },
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "for input_text,target_text in zip(train_df[1][0:],train_df[0][0:]):\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"  #appending start and end tokens\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "#getting the list of input characters and making a dictionary for ease of processing\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "pred_target_token_index = dict([(i, char) for i, char in enumerate(target_characters)])\n",
        "\n",
        "#adding the space token to both input and target dictionaries\n",
        "if ' ' not in input_token_index:\n",
        "  input_token_index[' '] = len(input_token_index)\n",
        "\n",
        "if ' ' not in target_token_index:\n",
        "  target_token_index[' '] = len(target_token_index)\n",
        "\n",
        "pred_target_token_index[len(pred_target_token_index)] = ' '\n",
        "\n",
        "\n",
        "num_encoder_tokens = len(input_characters) + 1\n",
        "num_decoder_tokens = len(target_characters) + 1\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "#getting the matrix representation of input and target data using the dictionaries defined earlier\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :] = target_token_index[\" \"]\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 68215\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hz1dWSErDM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387ee868-d7e7-4f35-f917-b10bfc9d080d"
      },
      "source": [
        "target_token_index"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\t': 0,\n",
              " '\\n': 1,\n",
              " ' ': 48,\n",
              " 'ஃ': 2,\n",
              " 'அ': 3,\n",
              " 'ஆ': 4,\n",
              " 'இ': 5,\n",
              " 'ஈ': 6,\n",
              " 'உ': 7,\n",
              " 'ஊ': 8,\n",
              " 'எ': 9,\n",
              " 'ஏ': 10,\n",
              " 'ஐ': 11,\n",
              " 'ஒ': 12,\n",
              " 'ஓ': 13,\n",
              " 'க': 14,\n",
              " 'ங': 15,\n",
              " 'ச': 16,\n",
              " 'ஜ': 17,\n",
              " 'ஞ': 18,\n",
              " 'ட': 19,\n",
              " 'ண': 20,\n",
              " 'த': 21,\n",
              " 'ந': 22,\n",
              " 'ன': 23,\n",
              " 'ப': 24,\n",
              " 'ம': 25,\n",
              " 'ய': 26,\n",
              " 'ர': 27,\n",
              " 'ற': 28,\n",
              " 'ல': 29,\n",
              " 'ள': 30,\n",
              " 'ழ': 31,\n",
              " 'வ': 32,\n",
              " 'ஷ': 33,\n",
              " 'ஸ': 34,\n",
              " 'ஹ': 35,\n",
              " 'ா': 36,\n",
              " 'ி': 37,\n",
              " 'ீ': 38,\n",
              " 'ு': 39,\n",
              " 'ூ': 40,\n",
              " 'ெ': 41,\n",
              " 'ே': 42,\n",
              " 'ை': 43,\n",
              " 'ொ': 44,\n",
              " 'ோ': 45,\n",
              " 'ௌ': 46,\n",
              " '்': 47}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTS-78fgSQ5a"
      },
      "source": [
        "## Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6AJbTIUY895"
      },
      "source": [
        "# Vectorize the data.\n",
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "for val_input_text,val_target_text in zip(val_df[1][0:],val_df[0][0:]):\n",
        "    val_target_text = \"\\t\" + val_target_text + \"\\n\"\n",
        "    val_input_texts.append(val_input_text)\n",
        "    val_target_texts.append(val_target_text)\n",
        "\n",
        "val_encoder_input_data = np.zeros(\n",
        "    (len(val_input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_input_data = np.zeros(\n",
        "    (len(val_input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_decoder_target_data = np.zeros(\n",
        "    (len(val_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t] = input_token_index[char]\n",
        "    val_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_decoder_input_data[i, t + 1 :] = target_token_index[\" \"]\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOMoZD-ySX1w"
      },
      "source": [
        "## Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYEIKYXy0-Lv"
      },
      "source": [
        "# Vectorize the data.\n",
        "test_input_texts = []\n",
        "test_target_texts = []\n",
        "\n",
        "for test_input_text,test_target_text in zip(test_df[1][0:],test_df[0][0:]):\n",
        "    test_target_text = \"\\t\" + test_target_text + \"\\n\"\n",
        "    test_input_texts.append(test_input_text)\n",
        "    test_target_texts.append(test_target_text)\n",
        "\n",
        "test_encoder_input_data = np.zeros(\n",
        "    (len(test_input_texts), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "test_decoder_input_data = np.zeros(\n",
        "    (len(test_input_texts), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "test_decoder_target_data = np.zeros(\n",
        "    (len(test_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(test_input_texts, test_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        test_encoder_input_data[i, t] = input_token_index[char]\n",
        "    test_encoder_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        test_decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            test_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    test_decoder_input_data[i, t + 1 :] = target_token_index[\" \"]\n",
        "    test_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2O2bazZ2R9E"
      },
      "source": [
        "# SWEEP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8HShe6A2Y2e"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7syg0ppVMhAl"
      },
      "source": [
        "def rnnmodel(cell_name, embedding_size, hidden_size, num_enc_layers, num_dec_layers, dropout, r_dropout, batch_size, num_epochs, optimizer):\n",
        "  \n",
        "  cell_type = { \"RNN\": keras.layers.SimpleRNN,\n",
        "                \"GRU\": keras.layers.GRU,\n",
        "                \"LSTM\": keras.layers.LSTM\n",
        "  }\n",
        "\n",
        "  learning_rate = 0.01\n",
        "  decay_rate = 0\n",
        "\n",
        "  optimizers = {\"adam\": tf.keras.optimizers.Adam(lr=learning_rate, decay=decay_rate),\n",
        "                \"nadam\": tf.keras.optimizers.Nadam(lr=learning_rate, decay=decay_rate),\n",
        "                \"rmsprop\": tf.keras.optimizers.RMSprop(lr=learning_rate, decay=decay_rate),\n",
        "                \"adagrad\": tf.keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
        "  }\n",
        "\n",
        "  # Define an input sequence and process it.\n",
        "  encoder_inputs = keras.layers.Input(shape=(None,), name=\"input_1\")\n",
        "  embedding = keras.layers.Embedding(num_encoder_tokens, embedding_size, name=\"embedding_1\")(encoder_inputs)\n",
        "  encoder_seq, *encoder_state = cell_type[cell_name](hidden_size, return_sequences=True,return_state = True, name=\"encoder_1\")(embedding)\n",
        "  for i in range(1, num_enc_layers):\n",
        "    encoder_seq, *encoder_state = cell_type[cell_name](hidden_size, return_sequences=True,return_state = True, name=\"encoder_\"+str(i+1), dropout=dropout,\n",
        "                                                       recurrent_dropout=r_dropout)(encoder_seq)\n",
        "\n",
        "  # Set up the decoder, using `encoder_states` as initial state.\n",
        "  decoder_inputs = keras.layers.Input(shape=(None,), name=\"input_2\")\n",
        "  decoder_embedding = keras.layers.Embedding(num_decoder_tokens, embedding_size, name=\"embedding_2\")(decoder_inputs)\n",
        "\n",
        "  # We set up our decoder to return full output sequences,\n",
        "  # and to return internal states as well. We don't use the\n",
        "  # return states in the training model, but we will use them in inference.\n",
        "  decoder_seq, *_ = cell_type[cell_name](hidden_size,return_sequences=True, return_state=True, name=\"decoder_1\")(decoder_embedding,initial_state=encoder_state)\n",
        "  for i in range(1, num_dec_layers):\n",
        "    decoder_seq, *_ = cell_type[cell_name](hidden_size, return_sequences=True, return_state=True, name=\"decoder_\"+str(i+1),dropout=dropout,\n",
        "                                      recurrent_dropout=r_dropout)(decoder_seq,initial_state=encoder_state)\n",
        "\n",
        "  decoder_outputs = keras.layers.TimeDistributed(keras.layers.Dense(num_decoder_tokens, activation=\"softmax\"),name = 'time_distributed')(decoder_seq)\n",
        "  #decoder_outputs = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\", name=\"dense_1\")(decoder_seq)\n",
        "\n",
        "\n",
        "  # Define the model that will turn\n",
        "  # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=optimizers[optimizer], loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "  )\n",
        "  model.fit(\n",
        "      [encoder_input_data, decoder_input_data],\n",
        "      decoder_target_data,\n",
        "      batch_size=batch_size,\n",
        "      epochs=num_epochs,\n",
        "      validation_data=([val_encoder_input_data, val_decoder_input_data],\n",
        "                      val_decoder_target_data),\n",
        "      callbacks=[WandbCallback()]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2_KKj_52qWe"
      },
      "source": [
        "## Helper functions for sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M-JgdwhZ0iw"
      },
      "source": [
        "#----------------DEFINING FUNCTION FOR INFERENCE--------------------------------\n",
        "def decode_sequence(input_seq,encoder_model,decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    #print(input_seq)\n",
        "    states_value = encoder_model.predict([input_seq])\n",
        "    #print(len(states_value))\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        #print([target_seq] + states_value)\n",
        "        output_tokens, *decoder_states = decoder_model.predict([target_seq, states_value])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = decoder_states\n",
        "        #print(len(decoded_sentence))\n",
        "    decoded_sentence = decoded_sentence.strip('\\n')\n",
        "    decoded_sentence = decoded_sentence.strip(' ')\n",
        "    return decoded_sentence\n",
        "\n",
        "#-------CALCULATING WORD-LEVEL TEST ACCURACY BY SAMPLE-BY-SAMPLE INFERENCE------\n",
        "def calc_test_acc(model,config):\n",
        "\n",
        "  # Define sampling models\n",
        "  # Restore the model and construct the encoder and decoder.\n",
        "  latent_dim = config.hidden_size\n",
        "  encoder_inputs = model.input[0]  # input_1\n",
        "  encoder_seq,*encoder_states  = model.get_layer('encoder_{}'.format(config.num_enc_layers)).output\n",
        "  #encoder_seq,state_h_enc, state_c_enc = encoder_lstm2(encoder_seq)  # lstm_\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "  decoder_inputs = keras.layers.Input(shape=(1,), name=\"input_2\") # input_2\n",
        "  decoder_embed = model.get_layer('embedding_2')\n",
        "  decoder_input = decoder_embed(decoder_inputs)\n",
        "\n",
        "  if(config.cell_name == 'LSTM'):\n",
        "    decoder_state_input_h = keras.layers.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "    decoder_state_input_c = keras.layers.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  else:\n",
        "    decoder_states_inputs = keras.layers.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "\n",
        "  decoder_lstm1 = model.get_layer('decoder_1')\n",
        "  decoder_outputs, *decoder_states = decoder_lstm1(\n",
        "      decoder_input, initial_state=decoder_states_inputs\n",
        "  )\n",
        "\n",
        "  for i in range(config.num_dec_layers-1):\n",
        "    decoder_outputs, *decoder_states = model.get_layer('decoder_'+str(i+2))(\n",
        "        decoder_outputs,initial_state=decoder_states_inputs\n",
        "    )\n",
        "\n",
        "  decoder_dense = model.get_layer('time_distributed')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = keras.Model(\n",
        "      [decoder_inputs, decoder_states_inputs], [decoder_outputs] + decoder_states\n",
        "  )\n",
        "\n",
        "  f = open(\"predictions_vanilla1.txt\", \"a\")\n",
        "  g = open(\"correct_predictions_vanilla1.txt\", \"a\")\n",
        "  pred = []\n",
        "  count = 0\n",
        "  for i in range(test_df.shape[0]):\n",
        "    decoded_sentence = decode_sequence(test_encoder_input_data[i:i+1],encoder_model,decoder_model)\n",
        "    pred.append(decoded_sentence)\n",
        "    f.write(\"Input:\" + test_df[1][i] + \" True output:\" + test_df[0][i] + \" Predicted output:\" + decoded_sentence + '\\n')\n",
        "    print(i,pred[i])\n",
        "    if(pred[i] == test_df[0][i]):\n",
        "      count = count +1\n",
        "      g.write(\"Input:\" + test_df[1][i] + \" True output:\" + test_df[0][i] + \" Predicted output:\" + decoded_sentence + '\\n')\n",
        "  f.close()\n",
        "  g.close()\n",
        "  true_test_acc = count/test_df.shape[0]\n",
        "\n",
        "  return true_test_acc"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trdWx-nwyHdm"
      },
      "source": [
        "#--------------WORD-LEVEL VALIDATION ACCURACY FOR SWEEPING----------------------\n",
        "def word_level_acc(model):\n",
        "  val_results = model.predict([val_encoder_input_data,val_decoder_input_data])\n",
        "  val_pred = {}\n",
        "  for i in range(val_results.shape[0]):\n",
        "    for j in range(max_decoder_seq_length):\n",
        "      if(j==0):\n",
        "        val_pred[i] = pred_target_token_index[np.argmax(val_results[i],axis = 1)[j]]\n",
        "      else:\n",
        "        val_pred[i] = val_pred[i] + pred_target_token_index[np.argmax(val_results[i],axis = 1)[j]]\n",
        "      #print(pred_target_token_index[np.argmax(val_results[i],axis = 1)[j]])\n",
        "    val_pred[i] = val_pred[i].replace('\\n', '')\n",
        "    val_pred[i] = val_pred[i].replace(' ', '')\n",
        "\n",
        "  val_print= {}\n",
        "  count =0;\n",
        "  for i in range(val_results.shape[0]):\n",
        "    val_print[i] = [val_df[0][i],val_df[1][i],val_df[2][i],val_pred[i]]\n",
        "    if(val_df[0][i]==val_pred[i]):\n",
        "      count = count+1\n",
        "      print(val_print[i])\n",
        "  val_acc = count/val_results.shape[0]\n",
        "  return val_acc"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_LtE9S2xGW"
      },
      "source": [
        "## Setting up the sweep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb30jmpVS4ga"
      },
      "source": [
        "def swp():\n",
        "  #----------------DEFAULT HYPERPARAMETERS--------------------------------------\n",
        "  hyperparameter_defaults = dict(\n",
        "      cell_name = \"GRU\",\n",
        "      embedding_size = 32,\n",
        "      hidden_size = 64,\n",
        "      num_enc_layers = 1,\n",
        "      num_dec_layers = 1,\n",
        "      dropout = 0.5,\n",
        "      r_dropout = 0,\n",
        "      batch_size = 512,\n",
        "      num_epochs = 5,\n",
        "      optimizer = \"rmsprop\"\n",
        "  )\n",
        "\n",
        "  #---------------------RUNNING THE MODEL---------------------------------------\n",
        "  wandb.init(project=\"Assignment 3 without Attention\", config=hyperparameter_defaults)\n",
        "  config = wandb.config\n",
        "  wandb.run.name = \"{}_cell_{}_embSize_{}_hiddenSize_{}_encLayers_{}_decLayers_{}_dropout_{}_rDropout_{}_batchSize_{}_epochs_{}\".format(config.cell_name, \n",
        "                    config.embedding_size,config.hidden_size, config.num_enc_layers, config.num_dec_layers, config.dropout, config.r_dropout, config.batch_size, config.num_epochs, \n",
        "                    config.optimizer)\n",
        "  \n",
        "  enc_dec_model = rnnmodel(config.cell_name, config.embedding_size,config.hidden_size, config.num_enc_layers, config.num_dec_layers, config.dropout, config.r_dropout, config.batch_size, \n",
        "         config.num_epochs, config.optimizer)\n",
        "\n",
        "  #-------------PLOTTING WORD-LEVEL VALIDATION ACCURACY WITH TEACHER FORCING----\n",
        "  true_val_acc = word_level_acc(enc_dec_model)\n",
        "  wandb.log({'word_level_val_accuracy':  true_val_acc})\n",
        "\n",
        "  wandb.run.finish()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad61A5fFV8qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6490e3b-498f-4ed0-a784-caeffb0ea270"
      },
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"My Sweep\",\n",
        "  \"method\": \"random\",\n",
        "  \"project\": \"Assignment03 without Attention\",\n",
        "  \"metric\":{\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\":\"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"cell_name\": {\n",
        "            \"values\": ['RNN', 'LSTM', 'GRU']\n",
        "        },\n",
        "        \"embedding_size\": {\n",
        "            \"values\":[16, 32]\n",
        "        }, \n",
        "        \"hidden_size\": {\n",
        "            \"values\":[64, 128, 256]\n",
        "        },\n",
        "        \"num_enc_layers\":{\n",
        "            \"values\":[1, 2, 3]\n",
        "        },\n",
        "        \"num_dec_layers\":{\n",
        "            \"values\":[1, 2, 3]\n",
        "        },  \n",
        "        \"dropout\":{\n",
        "            \"values\":[0,  0.3,  0.5]\n",
        "        },\n",
        "        \"r_dropout\": {\n",
        "            \"values\":[0]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\":[32,64,128,256]\n",
        "        },\n",
        "        \"num_epochs\": {\n",
        "            \"values\":[10, 15, 20, 25]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\":['adam', 'nadam','rmsprop']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: cx0yek50\n",
            "Sweep URL: https://wandb.ai/kodikarthik21/uncategorized/sweeps/cx0yek50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTRZta3d9NCM"
      },
      "source": [
        "#wandb.agent('my4rog76', swp)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docsDatEhJNL"
      },
      "source": [
        "# BEST MODEL - TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "MykmclVPhIFK",
        "outputId": "773bc428-9a31-45ba-ef6a-54fb0e3282de"
      },
      "source": [
        "#------------CALCULATING WORD-LEVEL TEST ACCURACY BY SAMPLING-------------------\n",
        "model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/uncategorized/ey7xk90x\").name)\n",
        "\n",
        "hyperparameter_defaults = dict(\n",
        "      cell_name = \"LSTM\",\n",
        "      embedding_size = 32,\n",
        "      hidden_size = 128,\n",
        "      num_enc_layers = 2,\n",
        "      num_dec_layers = 3,\n",
        "      dropout = 0.5,\n",
        "      r_dropout = 0,\n",
        "      batch_size = 256,\n",
        "      num_epochs = 25,\n",
        "      optimizer = \"adam\"\n",
        ")\n",
        "\n",
        "wandb.init(project=\"Assignment 3 without Attention\", config=hyperparameter_defaults)\n",
        "config = wandb.config\n",
        "\n",
        "# test_acc = calc_test_acc(model,config)\n",
        "# print(test_acc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">confused-jazz-85</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/kodikarthik21/Assignment%203%20without%20Attention\" target=\"_blank\">https://wandb.ai/kodikarthik21/Assignment%203%20without%20Attention</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/kodikarthik21/Assignment%203%20without%20Attention/runs/27cnp3ao\" target=\"_blank\">https://wandb.ai/kodikarthik21/Assignment%203%20without%20Attention/runs/27cnp3ao</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210520_112437-27cnp3ao</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nsopEhs6DC6"
      },
      "source": [
        "#BEAM SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn-7bVR-5x3h"
      },
      "source": [
        "\n",
        "def beam_search(input_seq, k,model,config):\n",
        "  # Define sampling models\n",
        "  # Restore the model and construct the encoder and decoder.\n",
        "  latent_dim = config['hidden_size']\n",
        "  encoder_inputs = model.input[0]  # input_1\n",
        "  encoder_seq,*encoder_states  = model.get_layer('encoder_{}'.format(config['num_enc_layers'])).output\n",
        "  #encoder_seq,state_h_enc, state_c_enc = encoder_lstm2(encoder_seq)  # lstm_\n",
        "  encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "  decoder_inputs = keras.layers.Input(shape=(1,), name=\"input_2\") # input_2\n",
        "  decoder_embed = model.get_layer('embedding_2')\n",
        "  decoder_input = decoder_embed(decoder_inputs)\n",
        "\n",
        "  if(config['cell_name'] == 'LSTM'):\n",
        "    decoder_state_input_h = keras.layers.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "    decoder_state_input_c = keras.layers.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  else:\n",
        "    decoder_states_inputs = keras.layers.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "\n",
        "  decoder_lstm1 = model.get_layer('decoder_1')\n",
        "  decoder_outputs, *decoder_states = decoder_lstm1(\n",
        "      decoder_input, initial_state=decoder_states_inputs\n",
        "  )\n",
        "\n",
        "  for i in range(config['num_dec_layers']-1):\n",
        "    decoder_outputs, *decoder_states = model.get_layer('decoder_'+str(i+2))(\n",
        "        decoder_outputs,initial_state=decoder_states_inputs\n",
        "    )\n",
        "\n",
        "  decoder_dense = model.get_layer('time_distributed')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = keras.Model(\n",
        "      [decoder_inputs, decoder_states_inputs], [decoder_outputs] + decoder_states\n",
        "  )\n",
        "  \n",
        "#----------------           BEAM SEARCH-----------------------------------------\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  # Generate empty target sequence of length 1.\n",
        "  target_seq = np.zeros((k, 1, 1))\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  target_seq[:, 0, 0] = target_token_index[\"\\t\"]\n",
        "\n",
        "  # Sampling loop for a batch of sequences\n",
        "  # (to simplify, here we assume a batch of size 1).\n",
        "  stop_condition = False\n",
        "  sequences = [[list(), 0.0]]\n",
        "  states = []\n",
        "  pred_word = [\"\"]\n",
        "  for x in range(k-1):\n",
        "    sequences = sequences + [[list(), 0.0]]\n",
        "    pred_word = pred_word+[\"\"]\n",
        "  seq_len = 0\n",
        "\n",
        "  while not stop_condition:\n",
        "    all_candidates = list()\n",
        "    \n",
        "    if seq_len==0:\n",
        "      seq, score = sequences[0]\n",
        "      output_tokens, *decoder_states = decoder_model.predict([target_seq[0], states_value])\n",
        "      states_value = decoder_states\n",
        "      for j in range(output_tokens.shape[2]):\n",
        "        candidate = [seq + [j], score - log(output_tokens[0, -1, j])]\n",
        "        all_candidates.append(candidate)\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      indices = sorted(range(len(all_candidates)), key=lambda x: all_candidates[x][1])[:k]\n",
        "      sequences = ordered[:k]\n",
        "      target_seq = np.zeros((len(indices), 1, 1))\n",
        "      count =0\n",
        "\n",
        "      for x in range(k):\n",
        "        states.append(states_value)\n",
        "\n",
        "      for r in range(len(indices)):\n",
        "        sampled_char = pred_target_token_index[indices[r]%49]\n",
        "        target_seq[r, 0, 0] = indices[r]%49\n",
        "        if sampled_char != \"\\n\" :\n",
        "            count = 1;  \n",
        "      if count == 0 or seq_len > max_decoder_seq_length:\n",
        "        stop_condition = True\n",
        "\n",
        "    else:\n",
        "      for l in range(k):\n",
        "        seq, score = sequences[l]\n",
        "        output_tokens, *decoder_states = decoder_model.predict([target_seq[l], states[l]])\n",
        "        states[l] = decoder_states \n",
        "        for j in range(output_tokens.shape[2]):\n",
        "          candidate = [seq + [j], score - log(output_tokens[0, -1, j])]\n",
        "          all_candidates.append(candidate)\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      indices = sorted(range(len(all_candidates)), key=lambda x: all_candidates[x][1])[:k]\n",
        "      sequences = ordered[:k]\n",
        "      target_seq = np.zeros((len(indices), 1, 1))\n",
        "      count = 0;\n",
        "      for r in range(len(indices)):\n",
        "        sampled_char = pred_target_token_index[indices[r]%49]\n",
        "        target_seq[r, 0, 0] = indices[r]%49\n",
        "        if sampled_char != \"\\n\" :\n",
        "            count = 1;\n",
        "      if count == 0 or seq_len > max_decoder_seq_length:\n",
        "        stop_condition = True\n",
        "    seq_len += 1\n",
        "\n",
        "  for i in range(k):\n",
        "    #print(sequences[i][0])\n",
        "    for j in sequences[i][0]:\n",
        "      if pred_target_token_index[j]!= '\\n' and pred_target_token_index[j]!= ' ':\n",
        "        pred_word[i]  += pred_target_token_index[j]\n",
        "      else:\n",
        "        break;\n",
        "  return pred_word"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uarafQ9X6HvN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "d7b24048-1967-46d4-ec75-cfe35a2540b0"
      },
      "source": [
        "#-----------------RUNNING BEAM SEARCH ON THE BEST MODEL-------------------------\n",
        "model = tf.keras.models.load_model(wandb.restore('model-best.h5', run_path=\"kodikarthik21/uncategorized/ey7xk90x\").name)\n",
        "import random\n",
        "api = wandb.Api()\n",
        "run = api.run(\"kodikarthik21/uncategorized/ey7xk90x\")\n",
        "#------taking random 20 characters and performing beam search on them-----------\n",
        "for i in range(20):\n",
        "  index = random.randint(0, test_df.shape[0])\n",
        "  #s = beam_search(test_encoder_input_data[index:index+1],5,model,run.config)\n",
        "  #print(\"English word:\" + test_df[1][index] + \" True word: \" + test_df[0][index] + \" Predicted outputs: \", s)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English word:dun True word: டூன் Predicted outputs:  ['டன்', 'டுன்', 'டன்', 'டண்', 'டுண்']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-43ac02927c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_encoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"English word:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" True word: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Predicted outputs: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-897eba1c046e>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(input_seq, k, model, config)\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdecoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2972\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}